#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿè®¡å·²å¤„ç†çš„å¯¹è¯æ•°é‡
"""

import os
import torch
import sys
import time
sys.path.append('.')
import config

def count_processed_dialogues():
    """ç»Ÿè®¡å·²å¤„ç†çš„å¯¹è¯æ•°é‡"""
    print("ğŸ“Š ç»Ÿè®¡å·²å¤„ç†çš„å¯¹è¯æ•°é‡...")
    
    data_types = ["train", "valid", "test"]
    total_processed = 0
    
    for data_type in data_types:
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
        if not os.path.exists(chunk_dir):
            print(f"ğŸ“ {data_type}: ç›®å½•ä¸å­˜åœ¨")
            continue
            
        # æŸ¥æ‰¾æ‰€æœ‰chunkæ–‡ä»¶
        chunk_files = []
        for file in os.listdir(chunk_dir):
            if file.startswith("chunk_") and file.endswith(".pt"):
                try:
                    chunk_num = int(file.split('_')[1].split('.')[0])
                    chunk_files.append((chunk_num, file))
                except:
                    pass
        
        if not chunk_files:
            print(f"ğŸ“ {data_type}: æ— chunkæ–‡ä»¶")
            continue
            
        # æŒ‰ç¼–å·æ’åº
        chunk_files.sort(key=lambda x: x[0])
        
        print(f"\nğŸ“ {data_type} æ•°æ®é›†:")
        print(f"   ğŸ“¦ æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
        print(f"   ğŸ”¢ ç¼–å·èŒƒå›´: {chunk_files[0][0]} - {chunk_files[-1][0]}")
        
        # ç»Ÿè®¡å¯¹è¯æ•°é‡
        type_total = 0
        sample_chunks = []
        
        # é‡‡æ ·ç»Ÿè®¡ï¼ˆå‰5ä¸ªã€ä¸­é—´5ä¸ªã€å5ä¸ªï¼‰
        if len(chunk_files) <= 15:
            sample_chunks = chunk_files
        else:
            sample_chunks.extend(chunk_files[:5])  # å‰5ä¸ª
            mid_start = len(chunk_files) // 2 - 2
            sample_chunks.extend(chunk_files[mid_start:mid_start+5])  # ä¸­é—´5ä¸ª
            sample_chunks.extend(chunk_files[-5:])  # å5ä¸ª
        
        print(f"   ğŸ” é‡‡æ ·ç»Ÿè®¡ {len(sample_chunks)} ä¸ªchunkæ–‡ä»¶...")
        
        sample_total = 0
        sample_count = 0
        
        for chunk_num, chunk_file in sample_chunks:
            chunk_path = os.path.join(chunk_dir, chunk_file)
            try:
                data = torch.load(chunk_path, weights_only=True)
                dialogues_count = len(data)
                sample_total += dialogues_count
                sample_count += 1
                
                if sample_count <= 3:  # æ˜¾ç¤ºå‰3ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                    print(f"      chunk_{chunk_num}: {dialogues_count} ä¸ªå¯¹è¯")
                    
            except Exception as e:
                print(f"      âŒ chunk_{chunk_num} åŠ è½½å¤±è´¥: {e}")
        
        if sample_count > 0:
            # è®¡ç®—å¹³å‡å€¼å¹¶ä¼°ç®—æ€»æ•°
            avg_per_chunk = sample_total / sample_count
            estimated_total = int(avg_per_chunk * len(chunk_files))
            
            print(f"   ğŸ“ˆ é‡‡æ ·ç»Ÿè®¡:")
            print(f"      å¹³å‡æ¯chunk: {avg_per_chunk:.1f} ä¸ªå¯¹è¯")
            print(f"      ä¼°ç®—æ€»æ•°: {estimated_total:,} ä¸ªå¯¹è¯")
            
            type_total = estimated_total
        
        total_processed += type_total
        print(f"   âœ… {data_type} ä¼°ç®—æ€»è®¡: {type_total:,} ä¸ªå¯¹è¯")
    
    print(f"\nğŸ¯ æ€»è®¡å·²å¤„ç†: {total_processed:,} ä¸ªå¯¹è¯")
    return total_processed

def estimate_progress():
    """ä¼°ç®—å¤„ç†è¿›åº¦"""
    print(f"\nğŸ“Š å¤„ç†è¿›åº¦ä¼°ç®—...")
    
    # è¯»å–åŸå§‹æ•°æ®æ–‡ä»¶å¤§å°æ¥ä¼°ç®—æ€»é‡
    total_lines = 0
    processed_lines = 0
    
    data_info = {}
    
    for data_type in ["train", "valid", "test"]:
        data_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.jsonl")
        if os.path.exists(data_file):
            # ä¼°ç®—æ€»è¡Œæ•°ï¼ˆé€šè¿‡æ–‡ä»¶å¤§å°ï¼‰
            file_size = os.path.getsize(data_file)
            
            # é‡‡æ ·ä¼°ç®—æ¯è¡Œå¤§å°
            with open(data_file, 'r', encoding='utf-8') as f:
                sample_lines = []
                for i, line in enumerate(f):
                    if i >= 100:  # é‡‡æ ·å‰100è¡Œ
                        break
                    sample_lines.append(len(line.encode('utf-8')))
            
            if sample_lines:
                avg_line_size = sum(sample_lines) / len(sample_lines)
                estimated_lines = int(file_size / avg_line_size)
                total_lines += estimated_lines
                
                data_info[data_type] = {
                    'file_size_mb': file_size / (1024 * 1024),
                    'estimated_lines': estimated_lines,
                    'avg_line_size': avg_line_size
                }
                
                print(f"   ğŸ“„ {data_type}.jsonl:")
                print(f"      æ–‡ä»¶å¤§å°: {file_size / (1024 * 1024):.1f} MB")
                print(f"      ä¼°ç®—è¡Œæ•°: {estimated_lines:,}")
    
    # æ£€æŸ¥å·²å¤„ç†çš„chunkæ•°é‡
    train_chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, "train")
    if os.path.exists(train_chunk_dir):
        chunk_files = [f for f in os.listdir(train_chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]
        if chunk_files:
            max_chunk = max([int(f.split('_')[1].split('.')[0]) for f in chunk_files])
            print(f"\n   ğŸ“¦ trainå·²å®Œæˆchunk: 0 - {max_chunk} ({max_chunk + 1} ä¸ª)")
            
            # åŸºäºchunkæ•°é‡ä¼°ç®—å·²å¤„ç†è¡Œæ•°
            if 'train' in data_info:
                total_train_lines = data_info['train']['estimated_lines']
                processed_ratio = (max_chunk + 1) / (total_train_lines / 10000)  # å‡è®¾æ¯chunkçº¦10000è¡Œ
                estimated_processed = int(total_train_lines * min(processed_ratio, 1.0))
                
                print(f"   ğŸ“ˆ ä¼°ç®—å·²å¤„ç†: {estimated_processed:,} è¡Œ")
                print(f"   ğŸ“Š å¤„ç†è¿›åº¦: {estimated_processed / total_train_lines * 100:.1f}%")
                
                processed_lines = estimated_processed
    
    return total_lines, processed_lines

def check_current_processing_status():
    """æ£€æŸ¥å½“å‰å¤„ç†çŠ¶æ€"""
    print(f"\nğŸ”„ å½“å‰å¤„ç†çŠ¶æ€æ£€æŸ¥...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¿›ç¨‹åœ¨è¿è¡Œ
    try:
        import psutil
        python_processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if proc.info['name'] and 'python' in proc.info['name'].lower():
                    cmdline = proc.info['cmdline']
                    if cmdline and any('prepare_binary_data' in str(cmd) for cmd in cmdline):
                        python_processes.append(proc.info)
            except:
                pass
        
        if python_processes:
            print(f"   ğŸ”„ å‘ç° {len(python_processes)} ä¸ªæ•°æ®å¤„ç†è¿›ç¨‹æ­£åœ¨è¿è¡Œ")
            for proc in python_processes:
                print(f"      PID: {proc['pid']}")
        else:
            print(f"   â¸ï¸  å½“å‰æ— æ•°æ®å¤„ç†è¿›ç¨‹è¿è¡Œ")
            
    except ImportError:
        print(f"   âš ï¸  psutilæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥è¿›ç¨‹çŠ¶æ€")
    
    # æ£€æŸ¥æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶
    train_chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, "train")
    if os.path.exists(train_chunk_dir):
        chunk_files = [f for f in os.listdir(train_chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]
        if chunk_files:
            # æ‰¾åˆ°æœ€æ–°çš„æ–‡ä»¶
            latest_file = None
            latest_time = 0
            
            for chunk_file in chunk_files:
                file_path = os.path.join(train_chunk_dir, chunk_file)
                mtime = os.path.getmtime(file_path)
                if mtime > latest_time:
                    latest_time = mtime
                    latest_file = chunk_file
            
            if latest_file:
                import datetime
                latest_time_str = datetime.datetime.fromtimestamp(latest_time).strftime('%Y-%m-%d %H:%M:%S')
                print(f"   ğŸ“… æœ€æ–°chunkæ–‡ä»¶: {latest_file}")
                print(f"   â° æœ€åä¿®æ”¹æ—¶é—´: {latest_time_str}")

def main():
    print("ğŸ“Š å¤„ç†è¿›åº¦ç»Ÿè®¡å·¥å…·")
    print("=" * 50)
    
    # 1. ç»Ÿè®¡å·²å¤„ç†çš„å¯¹è¯æ•°é‡
    processed_dialogues = count_processed_dialogues()
    
    # 2. ä¼°ç®—å¤„ç†è¿›åº¦
    total_lines, processed_lines = estimate_progress()
    
    # 3. æ£€æŸ¥å½“å‰çŠ¶æ€
    check_current_processing_status()
    
    # 4. æ€»ç»“
    print(f"\nğŸ¯ å¤„ç†è¿›åº¦æ€»ç»“:")
    print(f"   ğŸ“Š å·²å¤„ç†å¯¹è¯æ•°: {processed_dialogues:,}")
    if total_lines > 0 and processed_lines > 0:
        progress_percent = processed_lines / total_lines * 100
        print(f"   ğŸ“ˆ ä¼°ç®—å¤„ç†è¿›åº¦: {progress_percent:.1f}%")
        print(f"   ğŸ“„ å·²å¤„ç†è¡Œæ•°: {processed_lines:,} / {total_lines:,}")
        
        remaining_lines = total_lines - processed_lines
        print(f"   â³ å‰©ä½™è¡Œæ•°: {remaining_lines:,}")
        
        # ä¼°ç®—å‰©ä½™æ—¶é—´ï¼ˆåŸºäºä¹‹å‰çš„å¤„ç†é€Ÿåº¦ï¼‰
        if processed_lines > 0:
            # å‡è®¾å¹³å‡å¤„ç†é€Ÿåº¦ä¸º200 it/s
            estimated_speed = 200
            remaining_seconds = remaining_lines / estimated_speed
            remaining_hours = remaining_seconds / 3600
            
            print(f"   â±ï¸  ä¼°ç®—å‰©ä½™æ—¶é—´: {remaining_hours:.1f} å°æ—¶ (å‡è®¾200 it/s)")

if __name__ == "__main__":
    main()
