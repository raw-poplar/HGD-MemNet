#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å½“å‰è®¾ç½®
"""

import os
import json
import sys
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
import config
from src.dataset import Vocabulary

def test_current_setup():
    """æµ‹è¯•å½“å‰è®¾ç½®æ˜¯å¦æ­£ç¡®"""
    print("ğŸ” æµ‹è¯•å½“å‰è®¾ç½®...")
    
    # 1. æ£€æŸ¥è·¯å¾„é…ç½®
    print(f"ğŸ“ LCCC_PROCESSED_PATH: {config.LCCC_PROCESSED_PATH}")
    
    # 2. æ£€æŸ¥è¯æ±‡è¡¨æ–‡ä»¶
    vocab_path = os.path.join(config.LCCC_PROCESSED_PATH, "vocabulary.json")
    print(f"ğŸ“š è¯æ±‡è¡¨è·¯å¾„: {vocab_path}")
    print(f"ğŸ“š è¯æ±‡è¡¨å­˜åœ¨: {os.path.exists(vocab_path)}")
    
    if os.path.exists(vocab_path):
        try:
            with open(vocab_path, 'r', encoding='utf-8') as f:
                vocab_dict = json.load(f)
            vocab = Vocabulary("lccc")
            vocab.__dict__.update(vocab_dict)
            print(f"âœ… è¯æ±‡è¡¨åŠ è½½æˆåŠŸï¼Œå¤§å°: {vocab.num_words}")
        except Exception as e:
            print(f"âŒ è¯æ±‡è¡¨åŠ è½½å¤±è´¥: {e}")
            return False
    else:
        print(f"âŒ è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    # 3. æ£€æŸ¥æ•°æ®æ–‡ä»¶
    data_types = ["train", "valid", "test"]
    for data_type in data_types:
        data_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.jsonl")
        print(f"ğŸ“„ {data_type}.jsonl å­˜åœ¨: {os.path.exists(data_file)}")
        
        if os.path.exists(data_file):
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            size_mb = os.path.getsize(data_file) / (1024 * 1024)
            print(f"   æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¯»å–
            try:
                with open(data_file, 'r', encoding='utf-8') as f:
                    first_line = f.readline()
                    if first_line:
                        dialogue = json.loads(first_line)
                        print(f"   âœ… å¯ä»¥è¯»å–ï¼Œæ ·æœ¬å¯¹è¯é•¿åº¦: {len(dialogue)}")
                    else:
                        print(f"   âš ï¸  æ–‡ä»¶ä¸ºç©º")
            except Exception as e:
                print(f"   âŒ è¯»å–å¤±è´¥: {e}")
    
    # 4. æ£€æŸ¥ç°æœ‰chunkæ–‡ä»¶
    for data_type in data_types:
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
        if os.path.exists(chunk_dir):
            chunk_files = [f for f in os.listdir(chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]
            print(f"ğŸ“¦ {data_type} ç°æœ‰chunkæ•°: {len(chunk_files)}")
            
            if chunk_files:
                # æ‰¾åˆ°æœ€å¤§ç¼–å·
                chunk_numbers = []
                for f in chunk_files:
                    try:
                        num = int(f.split('_')[1].split('.')[0])
                        chunk_numbers.append(num)
                    except:
                        pass
                
                if chunk_numbers:
                    max_chunk = max(chunk_numbers)
                    print(f"   æœ€å¤§chunkç¼–å·: {max_chunk}")
                    print(f"   ä¸‹ä¸€ä¸ªchunkå°†æ˜¯: chunk_{max_chunk + 1}.pt")
        else:
            print(f"ğŸ“¦ {data_type} chunkç›®å½•ä¸å­˜åœ¨")
    
    # 5. æ£€æŸ¥resumeæ–‡ä»¶
    for data_type in data_types:
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
        resume_file = os.path.join(chunk_dir, f"resume_{data_type}.json")
        if os.path.exists(resume_file):
            try:
                with open(resume_file, 'r') as f:
                    resume_data = json.load(f)
                print(f"ğŸ”„ {data_type} resumeä¿¡æ¯: {resume_data}")
            except Exception as e:
                print(f"ğŸ”„ {data_type} resumeæ–‡ä»¶æŸå: {e}")
        else:
            print(f"ğŸ”„ {data_type} æ— resumeæ–‡ä»¶ï¼ˆå°†ä»å¤´å¼€å§‹ï¼‰")
    
    return True

def estimate_processing_time():
    """ä¼°ç®—å¤„ç†æ—¶é—´"""
    print(f"\nâ±ï¸  å¤„ç†æ—¶é—´ä¼°ç®—:")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    total_size = 0
    for data_type in ["train", "valid", "test"]:
        data_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.jsonl")
        if os.path.exists(data_file):
            size_mb = os.path.getsize(data_file) / (1024 * 1024)
            total_size += size_mb
            print(f"   {data_type}: {size_mb:.1f} MB")
    
    print(f"   æ€»å¤§å°: {total_size:.1f} MB")
    
    # åŸºäºç»éªŒä¼°ç®—
    # å‡è®¾å¤„ç†é€Ÿåº¦ä¸º 50-200 it/sï¼Œæ¯ä¸ªå¯¹è¯çº¦ 0.5KB
    estimated_dialogues = total_size * 1024 / 0.5  # ä¼°ç®—å¯¹è¯æ•°
    
    speeds = {
        "å•çº¿ç¨‹": 100,
        "4çº¿ç¨‹": 300,
        "8çº¿ç¨‹": 400
    }
    
    print(f"   ä¼°ç®—å¯¹è¯æ•°: {estimated_dialogues:,.0f}")
    print(f"   é¢„è®¡å¤„ç†æ—¶é—´:")
    
    for mode, speed in speeds.items():
        time_seconds = estimated_dialogues / speed
        time_minutes = time_seconds / 60
        time_hours = time_minutes / 60
        
        if time_hours > 1:
            print(f"     {mode}: {time_hours:.1f} å°æ—¶")
        elif time_minutes > 1:
            print(f"     {mode}: {time_minutes:.1f} åˆ†é’Ÿ")
        else:
            print(f"     {mode}: {time_seconds:.1f} ç§’")

if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•å½“å‰è®¾ç½®...")
    
    if test_current_setup():
        print(f"\nâœ… è®¾ç½®æ£€æŸ¥é€šè¿‡ï¼")
        estimate_processing_time()
        
        print(f"\nğŸ’¡ å»ºè®®:")
        print(f"1. ä½¿ç”¨ 4-8 ä¸ªworkerè¿›ç¨‹ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        print(f"2. ç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œç¡®ä¿ä¸è¶…è¿‡ç³»ç»Ÿé™åˆ¶")
        print(f"3. å¤„ç†è¿‡ç¨‹ä¸­å¯ä»¥éšæ—¶ä¸­æ–­ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ")
        print(f"4. å®Œæˆåä¼šè‡ªåŠ¨åˆå¹¶ä¸ºæœ€ç»ˆçš„train/valid/testæ–‡ä»¶")
    else:
        print(f"\nâŒ è®¾ç½®æ£€æŸ¥å¤±è´¥ï¼")
        print(f"è¯·æ£€æŸ¥è·¯å¾„é…ç½®å’Œæ–‡ä»¶å®Œæ•´æ€§")
