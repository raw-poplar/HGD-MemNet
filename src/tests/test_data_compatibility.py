#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å…¼å®¹æ€§æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¿®æ”¹å‰åçš„æ•°æ®æ ¼å¼æ˜¯å¦å…¼å®¹
"""

import torch
import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
import config
from src.dataset import Vocabulary, binary_collate_fn

def create_old_format_data():
    """åˆ›å»ºä¿®æ”¹å‰çš„æ•°æ®æ ¼å¼ï¼ˆæ€è€ƒæ­¥éª¤ä¸­x_tä½¿ç”¨å‰ä¸€å¥è¯ï¼‰"""
    vocab = Vocabulary("test")
    vocab.addSentence("hello world")
    vocab.addSentence("how are you")
    vocab.addSentence("I am fine")
    
    def indexesFromSentence(vocab, sentence):
        if sentence is None:
            return []
        UNK_idx = vocab.word2index.get("<UNK>", config.UNK_token)
        return [vocab.word2index.get(word, UNK_idx) for word in sentence.split(' ')] + [config.EOS_token]
    
    # æ¨¡æ‹Ÿä¿®æ”¹å‰çš„æ•°æ®å¤„ç†é€»è¾‘
    dialogue_list = [
        {"text": "hello world"},
        {"text": "how are you"},
        {"text": "I am fine"}
    ]
    
    x_ref_text = dialogue_list[0].get("text", "")
    x_ref_tensor = torch.tensor(indexesFromSentence(vocab, x_ref_text), dtype=torch.long)
    
    steps_data = []
    for i in range(1, len(dialogue_list)):
        x_t_text = dialogue_list[i-1].get("text", "")
        target_text = dialogue_list[i].get("text", "")
        
        # ä¿®æ”¹å‰çš„é€»è¾‘ï¼šæ€è€ƒæ­¥éª¤ä¸­x_tä½¿ç”¨å‰ä¸€å¥è¯
        for _ in range(config.THINKING_STEPS):
            thinking_step_x_t = torch.tensor(indexesFromSentence(vocab, x_t_text), dtype=torch.long)
            thinking_step_target = torch.tensor([], dtype=torch.long)
            thinking_step_gate = torch.tensor([0.0], dtype=torch.float)
            steps_data.append((thinking_step_x_t, thinking_step_target, thinking_step_gate))
        
        # è¾“å‡ºæ­¥éª¤
        output_step_x_t = torch.tensor(indexesFromSentence(vocab, x_t_text), dtype=torch.long)
        output_step_target = torch.tensor(indexesFromSentence(vocab, target_text), dtype=torch.long)
        output_step_gate = torch.tensor([1.0], dtype=torch.float)
        steps_data.append((output_step_x_t, output_step_target, output_step_gate))
    
    return (x_ref_tensor, steps_data)

def create_new_format_data():
    """åˆ›å»ºä¿®æ”¹åçš„æ•°æ®æ ¼å¼ï¼ˆæ€è€ƒæ­¥éª¤ä¸­x_tä½¿ç”¨Noneï¼‰"""
    from src.prepare_binary_data import process_dialogue_to_tensors
    
    vocab = Vocabulary("test")
    vocab.addSentence("hello world")
    vocab.addSentence("how are you")
    vocab.addSentence("I am fine")
    
    dialogue_list = [
        {"text": "hello world"},
        {"text": "how are you"},
        {"text": "I am fine"}
    ]
    
    return process_dialogue_to_tensors(dialogue_list, vocab)

def test_data_compatibility():
    """æµ‹è¯•æ•°æ®å…¼å®¹æ€§"""
    print("=== æ•°æ®å…¼å®¹æ€§æµ‹è¯• ===")
    
    # åˆ›å»ºä¸¤ç§æ ¼å¼çš„æ•°æ®
    old_data = create_old_format_data()
    new_data = create_new_format_data()
    
    print(f"ä¿®æ”¹å‰æ•°æ®æ ¼å¼:")
    x_ref_old, steps_old = old_data
    print(f"  x_ref shape: {x_ref_old.shape}")
    print(f"  steps count: {len(steps_old)}")
    
    print(f"ä¿®æ”¹åæ•°æ®æ ¼å¼:")
    x_ref_new, steps_new = new_data
    print(f"  x_ref shape: {x_ref_new.shape}")
    print(f"  steps count: {len(steps_new)}")
    
    # æ£€æŸ¥åŸºæœ¬ç»“æ„å…¼å®¹æ€§
    assert x_ref_old.shape == x_ref_new.shape, "x_refå½¢çŠ¶ä¸å…¼å®¹"
    assert len(steps_old) == len(steps_new), "æ­¥éª¤æ•°é‡ä¸å…¼å®¹"
    
    print("\n=== æ­¥éª¤è¯¦ç»†å¯¹æ¯” ===")
    for i, ((x_t_old, target_old, gate_old), (x_t_new, target_new, gate_new)) in enumerate(zip(steps_old, steps_new)):
        print(f"æ­¥éª¤ {i}:")
        print(f"  ä¿®æ”¹å‰: x_t={x_t_old.shape if x_t_old is not None else None}, target={target_old.shape}, gate={gate_old.item()}")
        print(f"  ä¿®æ”¹å: x_t={x_t_new.shape if x_t_new is not None else None}, target={target_new.shape}, gate={gate_new.item()}")
        
        # æ£€æŸ¥ç›®æ ‡å’Œé—¨æ§æ˜¯å¦ä¸€è‡´
        assert torch.equal(target_old, target_new), f"æ­¥éª¤{i}ç›®æ ‡ä¸ä¸€è‡´"
        assert torch.equal(gate_old, gate_new), f"æ­¥éª¤{i}é—¨æ§ä¸ä¸€è‡´"
    
    print("\n=== æ‰¹å¤„ç†å…¼å®¹æ€§æµ‹è¯• ===")
    # æµ‹è¯•æ‰¹å¤„ç†å‡½æ•°æ˜¯å¦èƒ½å¤„ç†ä¸¤ç§æ ¼å¼
    batch_old = [old_data, old_data]  # é‡å¤æ•°æ®æ¨¡æ‹Ÿæ‰¹æ¬¡
    batch_new = [new_data, new_data]
    
    try:
        x_ref_batch_old, steps_batch_old = binary_collate_fn(batch_old)
        print("âœ… ä¿®æ”¹å‰æ ¼å¼å¯ä»¥æ­£å¸¸æ‰¹å¤„ç†")
    except Exception as e:
        print(f"âŒ ä¿®æ”¹å‰æ ¼å¼æ‰¹å¤„ç†å¤±è´¥: {e}")
        return False
    
    try:
        x_ref_batch_new, steps_batch_new = binary_collate_fn(batch_new)
        print("âœ… ä¿®æ”¹åæ ¼å¼å¯ä»¥æ­£å¸¸æ‰¹å¤„ç†")
    except Exception as e:
        print(f"âŒ ä¿®æ”¹åæ ¼å¼æ‰¹å¤„ç†å¤±è´¥: {e}")
        return False
    
    # æ£€æŸ¥æ‰¹å¤„ç†ç»“æœçš„å…¼å®¹æ€§
    assert x_ref_batch_old.shape == x_ref_batch_new.shape, "æ‰¹å¤„ç†x_refå½¢çŠ¶ä¸å…¼å®¹"
    assert len(steps_batch_old) == len(steps_batch_new), "æ‰¹å¤„ç†æ­¥éª¤æ•°ä¸å…¼å®¹"
    
    print("\n=== æ··åˆæ‰¹æ¬¡æµ‹è¯• ===")
    # æµ‹è¯•æ··åˆæ–°æ—§æ ¼å¼çš„æ‰¹æ¬¡
    mixed_batch = [old_data, new_data]
    try:
        x_ref_mixed, steps_mixed = binary_collate_fn(mixed_batch)
        print("âœ… æ··åˆæ ¼å¼æ‰¹æ¬¡å¯ä»¥æ­£å¸¸å¤„ç†")
        print(f"  æ··åˆæ‰¹æ¬¡ x_ref shape: {x_ref_mixed.shape}")
        print(f"  æ··åˆæ‰¹æ¬¡ steps count: {len(steps_mixed)}")
    except Exception as e:
        print(f"âŒ æ··åˆæ ¼å¼æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
        return False
    
    return True

def test_chunk_loading():
    """æµ‹è¯•ç°æœ‰chunkæ–‡ä»¶çš„åŠ è½½"""
    print("\n=== Chunkæ–‡ä»¶åŠ è½½æµ‹è¯• ===")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰çš„chunkæ–‡ä»¶
    chunk_dirs = []
    for data_type in ["train", "valid", "test"]:
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
        if os.path.exists(chunk_dir):
            chunk_files = [f for f in os.listdir(chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]
            if chunk_files:
                chunk_dirs.append((data_type, chunk_dir, len(chunk_files)))
    
    if not chunk_dirs:
        print("âš ï¸  æœªæ‰¾åˆ°ç°æœ‰çš„chunkæ–‡ä»¶")
        return True
    
    for data_type, chunk_dir, count in chunk_dirs:
        print(f"å‘ç° {data_type} æ•°æ®: {count} ä¸ªchunkæ–‡ä»¶")
        
        # å°è¯•åŠ è½½ç¬¬ä¸€ä¸ªchunk
        first_chunk = os.path.join(chunk_dir, "chunk_0.pt")
        if os.path.exists(first_chunk):
            try:
                data = torch.load(first_chunk, weights_only=True)
                print(f"  âœ… æˆåŠŸåŠ è½½ {first_chunk}")
                print(f"  ğŸ“Š åŒ…å« {len(data)} ä¸ªå¯¹è¯")
                
                # æ£€æŸ¥æ•°æ®æ ¼å¼
                if data:
                    sample = data[0]
                    x_ref, steps_data = sample
                    print(f"  ğŸ“ æ ·æœ¬æ ¼å¼: x_ref={x_ref.shape}, steps={len(steps_data)}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰Noneçš„x_tï¼ˆæ–°æ ¼å¼ï¼‰æˆ–éƒ½æœ‰å€¼ï¼ˆæ—§æ ¼å¼ï¼‰
                    none_count = sum(1 for x_t, _, _ in steps_data if x_t is None)
                    total_steps = len(steps_data)
                    print(f"  ğŸ” None x_t æ¯”ä¾‹: {none_count}/{total_steps} ({none_count/total_steps*100:.1f}%)")
                    
                    if none_count > 0:
                        print(f"  ğŸ“‹ æ£€æµ‹åˆ°æ–°æ ¼å¼æ•°æ®ï¼ˆåŒ…å«None x_tï¼‰")
                    else:
                        print(f"  ğŸ“‹ æ£€æµ‹åˆ°æ—§æ ¼å¼æ•°æ®ï¼ˆæ— None x_tï¼‰")
                        
            except Exception as e:
                print(f"  âŒ åŠ è½½å¤±è´¥: {e}")
                return False
    
    return True

if __name__ == "__main__":
    print("å¼€å§‹æ•°æ®å…¼å®¹æ€§æµ‹è¯•...")
    
    # æµ‹è¯•æ•°æ®æ ¼å¼å…¼å®¹æ€§
    if test_data_compatibility():
        print("\nâœ… æ•°æ®æ ¼å¼å…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
    else:
        print("\nâŒ æ•°æ®æ ¼å¼å…¼å®¹æ€§æµ‹è¯•å¤±è´¥")
        sys.exit(1)
    
    # æµ‹è¯•ç°æœ‰chunkæ–‡ä»¶
    if test_chunk_loading():
        print("\nâœ… Chunkæ–‡ä»¶åŠ è½½æµ‹è¯•é€šè¿‡")
    else:
        print("\nâŒ Chunkæ–‡ä»¶åŠ è½½æµ‹è¯•å¤±è´¥")
        sys.exit(1)
    
    print("\nğŸ‰ æ‰€æœ‰å…¼å®¹æ€§æµ‹è¯•é€šè¿‡ï¼")
    print("\nğŸ“‹ ç»“è®º:")
    print("1. ä¿®æ”¹åçš„ä»£ç å¯ä»¥å¤„ç†ä¿®æ”¹å‰çš„æ•°æ®æ ¼å¼")
    print("2. æ–°æ—§æ ¼å¼å¯ä»¥åœ¨åŒä¸€æ‰¹æ¬¡ä¸­æ··åˆå¤„ç†")
    print("3. ç°æœ‰çš„chunkæ–‡ä»¶å¯ä»¥æ­£å¸¸åŠ è½½")
    print("4. å¯ä»¥å®‰å…¨åœ°ç»§ç»­ä½¿ç”¨ä¿®æ”¹åçš„ä»£ç è¿›è¡Œæ•°æ®è½¬æ¢")
