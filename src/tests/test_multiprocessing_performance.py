#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šè¿›ç¨‹æ€§èƒ½æµ‹è¯•è„šæœ¬
"""

import time
import json
import os
import sys
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from src.dataset import Vocabulary
from src.prepare_binary_data import process_dialogue_to_tensors, init_worker, process_batch_optimized
from multiprocessing import Pool, cpu_count

def create_test_data(num_dialogues=1000):
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    test_dialogues = []
    for i in range(num_dialogues):
        dialogue = [
            {"text": f"hello world {i}"},
            {"text": f"how are you {i}"},
            {"text": f"I am fine {i}"},
            {"text": f"goodbye {i}"}
        ]
        test_dialogues.append(json.dumps(dialogue))
    return test_dialogues

def test_single_thread(test_data, vocab):
    """æµ‹è¯•å•çº¿ç¨‹æ€§èƒ½"""
    print("ğŸ”„ æµ‹è¯•å•çº¿ç¨‹æ€§èƒ½...")
    start_time = time.time()
    
    results = []
    for line in test_data:
        try:
            dialogue = json.loads(line)
            tensor_data = process_dialogue_to_tensors(dialogue, vocab)
            if tensor_data:
                results.append(tensor_data)
        except:
            pass
    
    end_time = time.time()
    duration = end_time - start_time
    speed = len(test_data) / duration
    
    print(f"   âœ… å•çº¿ç¨‹: {len(results)} ä¸ªå¯¹è¯, ç”¨æ—¶ {duration:.2f}s, é€Ÿåº¦ {speed:.1f} it/s")
    return results, speed

def test_new_multithread(test_data, vocab, num_workers=4):
    """æµ‹è¯•æ–°çš„å¤šçº¿ç¨‹å®ç°"""
    print(f"ğŸ”„ æµ‹è¯•æ–°å¤šçº¿ç¨‹æ€§èƒ½ (workers={num_workers})...")
    
    start_time = time.time()
    
    # å‡†å¤‡è¯æ±‡è¡¨çŠ¶æ€
    vocab_state = vocab.__dict__.copy()
    
    # åˆ†æ‰¹å¤„ç†
    BATCH_SIZE = max(100, len(test_data) // (num_workers * 4))
    batches = [test_data[i:i+BATCH_SIZE] for i in range(0, len(test_data), BATCH_SIZE)]
    
    with Pool(num_workers, initializer=init_worker, initargs=(vocab_state,)) as pool:
        batch_results = pool.map(process_batch_optimized, batches)
        results = []
        for batch_result in batch_results:
            results.extend(batch_result)
    
    end_time = time.time()
    duration = end_time - start_time
    speed = len(test_data) / duration
    
    print(f"   âœ… æ–°å¤šçº¿ç¨‹: {len(results)} ä¸ªå¯¹è¯, ç”¨æ—¶ {duration:.2f}s, é€Ÿåº¦ {speed:.1f} it/s")
    return results, speed

def test_scaling_performance():
    """æµ‹è¯•ä¸åŒworkeræ•°é‡çš„æ€§èƒ½"""
    print("\nğŸ“Š æµ‹è¯•ä¸åŒworkeræ•°é‡çš„æ€§èƒ½...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_test_data(2000)
    
    # åˆ›å»ºè¯æ±‡è¡¨
    vocab = Vocabulary("test")
    for line in test_data[:100]:  # åªç”¨å‰100ä¸ªå»ºç«‹è¯æ±‡è¡¨
        try:
            dialogue = json.loads(line)
            for item in dialogue:
                vocab.addSentence(item.get("text", ""))
        except:
            pass
    
    print(f"ğŸ“ æµ‹è¯•æ•°æ®: {len(test_data)} ä¸ªå¯¹è¯")
    print(f"ğŸ“š è¯æ±‡è¡¨å¤§å°: {vocab.num_words}")
    print(f"ğŸ’» å¯ç”¨CPU: {cpu_count()}")
    
    # æµ‹è¯•å•çº¿ç¨‹
    _, single_speed = test_single_thread(test_data, vocab)
    
    # æµ‹è¯•ä¸åŒworkeræ•°é‡
    worker_counts = [1, 2, 4, 8]
    new_speeds = []
    
    for workers in worker_counts:
        if workers <= cpu_count():
            print(f"\n--- Workers = {workers} ---")
            
            # æµ‹è¯•æ–°å®ç°
            try:
                _, new_speed = test_new_multithread(test_data, vocab, workers)
                new_speeds.append(new_speed)
            except Exception as e:
                print(f"   âŒ æ–°å¤šçº¿ç¨‹å¤±è´¥: {e}")
                new_speeds.append(0)
        else:
            new_speeds.append(0)
    
    # æ€§èƒ½æ€»ç»“
    print(f"\nğŸ“ˆ æ€§èƒ½æ€»ç»“:")
    print(f"{'Workers':<8} {'å•çº¿ç¨‹':<12} {'æ–°å¤šçº¿ç¨‹':<12} {'æå‡å€æ•°':<10}")
    print("-" * 50)
    
    for i, workers in enumerate(worker_counts):
        if workers <= cpu_count():
            new_speed = new_speeds[i] if i < len(new_speeds) else 0
            ratio = new_speed / single_speed if single_speed > 0 else 0
            
            print(f"{workers:<8} {single_speed:<12.1f} {new_speed:<12.1f} {ratio:<10.1f}x")
    
    return single_speed, new_speeds

def analyze_bottlenecks():
    """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
    print(f"\nğŸ” æ€§èƒ½ç“¶é¢ˆåˆ†æ:")
    
    # æµ‹è¯•åºåˆ—åŒ–å¼€é”€
    vocab = Vocabulary("test")
    vocab.addSentence("hello world test")
    
    import pickle
    start_time = time.time()
    for _ in range(1000):
        pickle.dumps(vocab)
    serialize_time = time.time() - start_time
    print(f"   ğŸ“¦ è¯æ±‡è¡¨åºåˆ—åŒ–å¼€é”€: {serialize_time*1000:.2f}ms/1000æ¬¡")
    
    # æµ‹è¯•JSONè§£æå¼€é”€
    test_line = json.dumps([{"text": "hello world"}, {"text": "how are you"}])
    start_time = time.time()
    for _ in range(1000):
        json.loads(test_line)
    json_time = time.time() - start_time
    print(f"   ğŸ“„ JSONè§£æå¼€é”€: {json_time*1000:.2f}ms/1000æ¬¡")
    
    # æµ‹è¯•å¼ é‡åˆ›å»ºå¼€é”€
    import torch
    start_time = time.time()
    for _ in range(1000):
        torch.tensor([1, 2, 3, 4, 5], dtype=torch.long)
    tensor_time = time.time() - start_time
    print(f"   ğŸ”¢ å¼ é‡åˆ›å»ºå¼€é”€: {tensor_time*1000:.2f}ms/1000æ¬¡")

if __name__ == "__main__":
    print("ğŸš€ å¤šè¿›ç¨‹æ€§èƒ½æµ‹è¯•å¼€å§‹...")
    
    try:
        # æ€§èƒ½æµ‹è¯•
        single_speed, new_speeds = test_scaling_performance()
        
        # ç“¶é¢ˆåˆ†æ
        analyze_bottlenecks()
        
        # ç»“è®º
        print(f"\nğŸ¯ ç»“è®º:")
        if new_speeds and max(new_speeds) > 0:
            best_new = max(new_speeds)
            
            if best_new > single_speed * 1.5:
                print(f"âœ… æ–°å¤šçº¿ç¨‹å®ç°æ˜¾è‘—ä¼˜äºå•çº¿ç¨‹ ({best_new:.1f} vs {single_speed:.1f} it/s)")
            elif best_new > single_speed * 1.2:
                print(f"âœ… æ–°å¤šçº¿ç¨‹å®ç°æœ‰æ•ˆæå‡æ€§èƒ½ ({best_new:.1f} vs {single_speed:.1f} it/s)")
            else:
                print(f"âš ï¸  å¤šçº¿ç¨‹æå‡æœ‰é™ï¼Œå»ºè®®ä½¿ç”¨å•çº¿ç¨‹")
                
            # æ¨èé…ç½®
            best_workers = new_speeds.index(max(new_speeds)) + 1
            if best_workers <= len([1, 2, 4, 8]):
                actual_workers = [1, 2, 4, 8][best_workers - 1]
                print(f"ğŸ’¡ æ¨èä½¿ç”¨ {actual_workers} ä¸ªworkerè¿›ç¨‹")
        else:
            print(f"âŒ å¤šçº¿ç¨‹æµ‹è¯•å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
