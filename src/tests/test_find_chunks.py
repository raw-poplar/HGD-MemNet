#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŸ¥æ‰¾ç°æœ‰çš„chunkæ–‡ä»¶
"""

import os
import glob
import torch
import sys
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

def find_chunk_files():
    """æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„chunkæ–‡ä»¶ä½ç½®"""
    print("æœç´¢chunkæ–‡ä»¶...")
    
    # å¯èƒ½çš„æœç´¢è·¯å¾„
    search_paths = [
        ".",
        "./data",
        "./data/lccc_processed",
        "F:/modelTrain/data/lccc_processed",  # åŸå§‹ç¡¬ç¼–ç è·¯å¾„
        "F:/modelTrain",
        "../data",
        "../../data",
    ]
    
    found_chunks = []
    
    for base_path in search_paths:
        if not os.path.exists(base_path):
            continue

        print(f"æœç´¢è·¯å¾„: {base_path}")
        
        # æœç´¢chunkæ–‡ä»¶
        patterns = [
            os.path.join(base_path, "**", "chunk_*.pt"),
            os.path.join(base_path, "chunk_*.pt"),
            os.path.join(base_path, "*", "chunk_*.pt"),
            os.path.join(base_path, "*", "*", "chunk_*.pt"),
        ]
        
        for pattern in patterns:
            files = glob.glob(pattern, recursive=True)
            for file in files:
                if file not in [f[0] for f in found_chunks]:
                    # è·å–æ–‡ä»¶ä¿¡æ¯
                    try:
                        stat = os.stat(file)
                        size_mb = stat.st_size / (1024 * 1024)
                        found_chunks.append((file, size_mb))
                    except:
                        found_chunks.append((file, 0))
    
    return found_chunks

def analyze_chunk_files(chunk_files):
    """åˆ†æchunkæ–‡ä»¶"""
    if not chunk_files:
        print("æœªæ‰¾åˆ°ä»»ä½•chunkæ–‡ä»¶")
        return None
    
    print(f"\næ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶:")
    
    # æŒ‰è·¯å¾„åˆ†ç»„
    by_directory = {}
    for file_path, size in chunk_files:
        directory = os.path.dirname(file_path)
        if directory not in by_directory:
            by_directory[directory] = []
        by_directory[directory].append((file_path, size))
    
    total_size = 0
    total_samples = 0
    
    for directory, files in by_directory.items():
        print(f"\nç›®å½•: {directory}")
        print(f"   æ–‡ä»¶æ•°é‡: {len(files)}")
        
        dir_size = sum(size for _, size in files)
        total_size += dir_size
        print(f"   æ€»å¤§å°: {dir_size:.2f} MB")
        
        # æ£€æŸ¥æ–‡ä»¶ç¼–å·è¿ç»­æ€§
        chunk_numbers = []
        for file_path, _ in files:
            filename = os.path.basename(file_path)
            try:
                # æå–chunkç¼–å·
                if filename.startswith("chunk_") and filename.endswith(".pt"):
                    num_str = filename[6:-3]  # å»æ‰"chunk_"å’Œ".pt"
                    chunk_numbers.append(int(num_str))
            except:
                pass
        
        if chunk_numbers:
            chunk_numbers.sort()
            print(f"   ç¼–å·èŒƒå›´: {min(chunk_numbers)} - {max(chunk_numbers)}")
            
            # æ£€æŸ¥æ˜¯å¦è¿ç»­
            expected = list(range(min(chunk_numbers), max(chunk_numbers) + 1))
            missing = set(expected) - set(chunk_numbers)
            if missing:
                print(f"   ç¼ºå¤±ç¼–å·: {sorted(missing)}")
            else:
                print(f"   ç¼–å·è¿ç»­")
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å†…å®¹
        if files:
            first_file = files[0][0]
            try:
                print(f"   æ£€æŸ¥æ–‡ä»¶: {os.path.basename(first_file)}")
                data = torch.load(first_file, weights_only=True)
                print(f"      æ ·æœ¬æ•°é‡: {len(data)}")
                total_samples += len(data) * len(files)  # ä¼°ç®—æ€»æ ·æœ¬æ•°
                
                if data:
                    sample = data[0]
                    x_ref, steps_data = sample
                    print(f"      æ ·æœ¬æ ¼å¼: x_ref={x_ref.shape}, steps={len(steps_data)}")
                    
                    # æ£€æŸ¥æ•°æ®æ ¼å¼ï¼ˆæ–°vsæ—§ï¼‰
                    none_count = sum(1 for x_t, _, _ in steps_data if x_t is None)
                    if none_count > 0:
                        print(f"      æ–°æ ¼å¼æ•°æ® (None x_t: {none_count}/{len(steps_data)})")
                    else:
                        print(f"      æ—§æ ¼å¼æ•°æ® (æ— None x_t)")
                        
            except Exception as e:
                print(f"      è¯»å–å¤±è´¥: {e}")
    
    print(f"\næ€»è®¡:")
    print(f"   æ–‡ä»¶æ€»æ•°: {len(chunk_files)}")
    print(f"   æ€»å¤§å°: {total_size:.2f} MB")
    print(f"   ä¼°è®¡æ ·æœ¬æ•°: {total_samples:,}")
    
    return by_directory

def check_continuation_possibility(chunk_dirs):
    """æ£€æŸ¥æ˜¯å¦å¯ä»¥ç»§ç»­å¤„ç†"""
    print(f"\nç»§ç»­å¤„ç†å¯è¡Œæ€§åˆ†æ:")
    
    for directory, files in chunk_dirs.items():
        print(f"\nğŸ“ {directory}:")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰partialæ–‡ä»¶
        partial_file = os.path.join(directory, "partial_processed.pt")
        if os.path.exists(partial_file):
            print(f"   æ‰¾åˆ°partialæ–‡ä»¶: {partial_file}")
            try:
                partial_data = torch.load(partial_file, weights_only=True)
                print(f"      Partialæ ·æœ¬æ•°: {len(partial_data)}")
            except Exception as e:
                print(f"      Partialæ–‡ä»¶æŸå: {e}")
        else:
            print(f"   æœªæ‰¾åˆ°partialæ–‡ä»¶")
        
        # æ£€æŸ¥æœ€å¤§chunkç¼–å·
        chunk_numbers = []
        for file_path, _ in files:
            filename = os.path.basename(file_path)
            try:
                if filename.startswith("chunk_") and filename.endswith(".pt"):
                    num_str = filename[6:-3]
                    chunk_numbers.append(int(num_str))
            except:
                pass
        
        if chunk_numbers:
            max_chunk = max(chunk_numbers)
            print(f"   æœ€å¤§chunkç¼–å·: {max_chunk}")
            print(f"   ä¸‹ä¸€ä¸ªchunkå°†æ˜¯: chunk_{max_chunk + 1}.pt")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰åˆå¹¶æ–‡ä»¶
        merged_files = []
        for split in ["train", "valid", "test"]:
            merged_file = os.path.join(directory, f"{split}_data.pt")
            if os.path.exists(merged_file):
                merged_files.append(split)
        
        if merged_files:
            print(f"   å·²å­˜åœ¨åˆå¹¶æ–‡ä»¶: {merged_files}")
            print(f"      å»ºè®®å¤‡ä»½åé‡æ–°åˆå¹¶")
        else:
            print(f"   æœªæ‰¾åˆ°åˆå¹¶æ–‡ä»¶ï¼Œå¯ä»¥æ­£å¸¸åˆå¹¶")

if __name__ == "__main__":
    print("æŸ¥æ‰¾ç°æœ‰chunkæ–‡ä»¶...")
    
    chunk_files = find_chunk_files()
    
    if chunk_files:
        chunk_dirs = analyze_chunk_files(chunk_files)
        if chunk_dirs:
            check_continuation_possibility(chunk_dirs)
            
            print(f"\nç»“è®º:")
            print(f"1. æ‰¾åˆ°äº†ç°æœ‰çš„chunkæ–‡ä»¶")
            print(f"2. ä¿®æ”¹åçš„ä»£ç å¯ä»¥ç»§ç»­å¤„ç†")
            print(f"3. æ•°æ®æ ¼å¼å…¼å®¹ï¼Œå¯ä»¥æ··åˆå¤„ç†")
            print(f"4. å¤„ç†å®Œæˆåå¯ä»¥æ­£å¸¸åˆå¹¶ä¸ºtrain/valid/testæ–‡ä»¶")
    else:
        print(f"\næœªæ‰¾åˆ°ä»»ä½•chunkæ–‡ä»¶")
        print(f"å¯èƒ½çš„åŸå› :")
        print(f"1. chunkæ–‡ä»¶åœ¨å…¶ä»–ä½ç½®")
        print(f"2. ä½¿ç”¨äº†ä¸åŒçš„æ–‡ä»¶åæ¨¡å¼")
        print(f"3. æ–‡ä»¶å·²è¢«ç§»åŠ¨æˆ–åˆ é™¤")
