#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åˆå¹¶å·¥å…·é›†åˆ

åŒ…å«å„ç§åˆå¹¶chunkæ–‡ä»¶çš„å·¥å…·å’Œæ–¹æ³•ï¼š
1. ä¼˜åŒ–ç‰ˆåˆå¹¶ - å†…å­˜å‹å¥½çš„æµå¼åˆå¹¶
2. ç®€å•åˆå¹¶ - åŸºç¡€çš„ä¸€æ¬¡æ€§åˆå¹¶
3. å¤§æ–‡ä»¶åˆå¹¶ - ä¸“é—¨å¤„ç†å¤§chunkæ–‡ä»¶
4. éªŒè¯å·¥å…· - æ£€æŸ¥åˆå¹¶ç»“æœ
"""

import os
import torch
import sys
import json
import gc
import time
import threading
import queue
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
import config

def get_chunk_files(data_type):
    """è·å–æŒ‡å®šæ•°æ®ç±»å‹çš„æ‰€æœ‰chunkæ–‡ä»¶"""
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    if not os.path.exists(chunk_dir):
        return []
    
    chunk_files = []
    for file in os.listdir(chunk_dir):
        if file.startswith("chunk_") and file.endswith(".pt"):
            try:
                chunk_num = int(file.split('_')[1].split('.')[0])
                chunk_files.append((chunk_num, file))
            except:
                pass
    
    return sorted(chunk_files, key=lambda x: x[0])

def merge_chunks_simple(data_type):
    """ç®€å•åˆå¹¶ - ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰chunk"""
    print(f"ğŸ”„ ç®€å•åˆå¹¶ {data_type}...")
    
    chunk_files = get_chunk_files(data_type)
    if not chunk_files:
        print(f"âŒ æœªæ‰¾åˆ°chunkæ–‡ä»¶")
        return False
    
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
    
    print(f"ğŸ“¦ æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    if os.path.exists(output_file):
        response = input(f"âš ï¸  {output_file} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–? (y/n): ")
        if response.lower() != 'y':
            return False
    
    # åˆå¹¶æ•°æ®
    all_data = []
    total_dialogues = 0
    
    for chunk_num, chunk_file in tqdm(chunk_files, desc=f"åˆå¹¶{data_type}"):
        chunk_path = os.path.join(chunk_dir, chunk_file)
        try:
            data = torch.load(chunk_path, weights_only=True)
            all_data.extend(data)
            total_dialogues += len(data)
            del data
            gc.collect()
        except Exception as e:
            print(f"âŒ åŠ è½½ {chunk_file} å¤±è´¥: {e}")
            return False
    
    # ä¿å­˜ç»“æœ
    print(f"ğŸ’¾ ä¿å­˜åˆ° {output_file}...")
    torch.save(all_data, output_file)
    
    # éªŒè¯
    file_size = os.path.getsize(output_file) / (1024**3)
    print(f"âœ… {data_type}.pt ä¿å­˜æˆåŠŸ")
    print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {total_dialogues:,}")
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} GB")
    
    return True

def merge_chunks_optimized(data_type, max_workers=2, batch_size=10):
    """ä¼˜åŒ–ç‰ˆåˆå¹¶ - æµå¼å¤„ç†ï¼Œå†…å­˜å‹å¥½"""
    print(f"ğŸ”„ ä¼˜åŒ–åˆå¹¶ {data_type} (æµå¼å¤„ç†)...")
    
    chunk_files = get_chunk_files(data_type)
    if not chunk_files:
        print(f"âŒ æœªæ‰¾åˆ°chunkæ–‡ä»¶")
        return False
    
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
    temp_file = output_file + ".tmp"
    
    print(f"ğŸ“¦ æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    if os.path.exists(output_file):
        response = input(f"âš ï¸  {output_file} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–? (y/n): ")
        if response.lower() != 'y':
            return False
    
    # åˆå§‹åŒ–ä¸´æ—¶æ–‡ä»¶
    torch.save([], temp_file)
    total_dialogues = 0
    
    def load_chunk_worker(chunk_info):
        """å·¥ä½œçº¿ç¨‹ï¼šåŠ è½½chunk"""
        chunk_num, chunk_file = chunk_info
        chunk_path = os.path.join(chunk_dir, chunk_file)
        try:
            data = torch.load(chunk_path, weights_only=True)
            return chunk_num, data, len(data)
        except Exception as e:
            print(f"âŒ åŠ è½½ {chunk_file} å¤±è´¥: {e}")
            return chunk_num, None, 0
    
    def append_to_temp_file(new_data):
        """è¿½åŠ æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶"""
        existing_data = torch.load(temp_file, weights_only=True)
        existing_data.extend(new_data)
        torch.save(existing_data, temp_file)
        del existing_data
        gc.collect()
    
    # åˆ†æ‰¹å¤„ç†
    with tqdm(total=len(chunk_files), desc=f"åˆå¹¶{data_type}") as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            for i in range(0, len(chunk_files), batch_size):
                batch = chunk_files[i:i+batch_size]
                
                # æäº¤åŠ è½½ä»»åŠ¡
                futures = [executor.submit(load_chunk_worker, chunk_info) for chunk_info in batch]
                
                # æ”¶é›†ç»“æœ
                batch_data = []
                for future in futures:
                    chunk_num, data, count = future.result()
                    if data is not None:
                        batch_data.extend(data)
                        total_dialogues += count
                        del data
                
                # è¿½åŠ åˆ°æ–‡ä»¶
                if batch_data:
                    append_to_temp_file(batch_data)
                    del batch_data
                    gc.collect()
                
                pbar.update(len(batch))
    
    # å®Œæˆåˆå¹¶
    if os.path.exists(output_file):
        os.remove(output_file)
    os.rename(temp_file, output_file)
    
    # éªŒè¯
    file_size = os.path.getsize(output_file) / (1024**3)
    print(f"âœ… {data_type}.pt ä¿å­˜æˆåŠŸ")
    print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {total_dialogues:,}")
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} GB")
    
    return True

def merge_chunks_large_files(data_type, timeout_seconds=300):
    """å¤§æ–‡ä»¶åˆå¹¶ - ä¸“é—¨å¤„ç†å¤§chunkæ–‡ä»¶"""
    print(f"ğŸ”„ å¤§æ–‡ä»¶åˆå¹¶ {data_type}...")
    
    chunk_files = get_chunk_files(data_type)
    if not chunk_files:
        print(f"âŒ æœªæ‰¾åˆ°chunkæ–‡ä»¶")
        return False
    
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
    
    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    total_size = 0
    for chunk_num, chunk_file in chunk_files:
        chunk_path = os.path.join(chunk_dir, chunk_file)
        size = os.path.getsize(chunk_path)
        size_mb = size / (1024*1024)
        total_size += size
        print(f"   ğŸ“„ {chunk_file}: {size_mb:.1f} MB")
    
    print(f"ğŸ“Š æ€»å¤§å°: {total_size / (1024*1024*1024):.2f} GB")
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    if os.path.exists(output_file):
        response = input(f"âš ï¸  {output_file} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–? (y/n): ")
        if response.lower() != 'y':
            return False
    
    # é€ä¸ªå¤„ç†å¤§æ–‡ä»¶
    all_data = []
    total_dialogues = 0
    
    for i, (chunk_num, chunk_file) in enumerate(chunk_files):
        chunk_path = os.path.join(chunk_dir, chunk_file)
        size_mb = os.path.getsize(chunk_path) / (1024*1024)
        
        print(f"\nğŸ“¦ å¤„ç† {chunk_file} ({i+1}/{len(chunk_files)})...")
        print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")
        
        start_time = time.time()
        
        try:
            print(f"   ğŸ”„ å¼€å§‹åŠ è½½... (è¶…æ—¶: {timeout_seconds}ç§’)")
            data = torch.load(chunk_path, weights_only=True)
            
            load_time = time.time() - start_time
            dialogue_count = len(data)
            
            print(f"   âœ… åŠ è½½æˆåŠŸ!")
            print(f"   â±ï¸  åŠ è½½æ—¶é—´: {load_time:.1f}ç§’")
            print(f"   ğŸ“Š å¯¹è¯æ•°: {dialogue_count:,}")
            
            # åˆå¹¶æ•°æ®
            all_data.extend(data)
            total_dialogues += dialogue_count
            
            print(f"   ğŸ“Š ç´¯è®¡å¯¹è¯æ•°: {total_dialogues:,}")
            
            # æ¸…ç†å†…å­˜
            del data
            gc.collect()
            
        except Exception as e:
            load_time = time.time() - start_time
            print(f"   âŒ åŠ è½½å¤±è´¥ (ç”¨æ—¶ {load_time:.1f}ç§’): {e}")
            
            response = input(f"   æ˜¯å¦è·³è¿‡æ­¤æ–‡ä»¶ç»§ç»­? (y/n): ")
            if response.lower() != 'y':
                return False
            continue
    
    # ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœ...")
    save_start = time.time()
    torch.save(all_data, output_file)
    save_time = time.time() - save_start
    
    print(f"âœ… ä¿å­˜æˆåŠŸ!")
    print(f"â±ï¸  ä¿å­˜æ—¶é—´: {save_time:.1f}ç§’")
    
    # éªŒè¯
    file_size = os.path.getsize(output_file) / (1024**3)
    print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶å¤§å°: {file_size:.2f} GB")
    print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {total_dialogues:,}")
    
    return True

def verify_merged_files():
    """éªŒè¯åˆå¹¶åçš„æ–‡ä»¶"""
    print("ğŸ” éªŒè¯åˆå¹¶åçš„æ–‡ä»¶...")
    
    datasets = ["train", "valid", "test"]
    
    for dataset in datasets:
        output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{dataset}.pt")
        
        if os.path.exists(output_file):
            try:
                print(f"\nğŸ“„ éªŒè¯ {dataset}.pt...")
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(output_file) / (1024**3)
                print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} GB")
                
                # åŠ è½½å¹¶æ£€æŸ¥æ•°æ®
                data = torch.load(output_file, weights_only=True)
                print(f"   ğŸ“Š å¯¹è¯æ•°é‡: {len(data):,}")
                
                # æ£€æŸ¥æ•°æ®ç»“æ„
                if data:
                    sample = data[0]
                    if isinstance(sample, tuple) and len(sample) == 2:
                        x_ref, steps = sample
                        print(f"   âœ… æ•°æ®ç»“æ„æ­£ç¡®")
                        print(f"   ğŸ“Š æ ·æœ¬: x_ref={x_ref.shape}, steps={len(steps)}")
                    else:
                        print(f"   âš ï¸  æ•°æ®ç»“æ„å¼‚å¸¸: {type(sample)}")
                
                del data
                gc.collect()
                
                print(f"   âœ… {dataset}.pt éªŒè¯é€šè¿‡")
                
            except Exception as e:
                print(f"   âŒ {dataset}.pt éªŒè¯å¤±è´¥: {e}")
        else:
            print(f"   âŒ {dataset}.pt ä¸å­˜åœ¨")

def cleanup_chunk_files(data_type, confirm=True):
    """æ¸…ç†chunkæ–‡ä»¶ï¼ˆåœ¨åˆå¹¶å®Œæˆåï¼‰"""
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    if not os.path.exists(chunk_dir):
        return
    
    chunk_files = [f for f in os.listdir(chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]
    
    if not chunk_files:
        print(f"ğŸ“ {data_type}: æ— chunkæ–‡ä»¶éœ€è¦æ¸…ç†")
        return
    
    print(f"ğŸ“ {data_type}: æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
    
    if confirm:
        response = input(f"æ˜¯å¦åˆ é™¤ {data_type} çš„chunkæ–‡ä»¶é‡Šæ”¾ç©ºé—´? (y/n): ")
        if response.lower() != 'y':
            return
    
    # åˆ é™¤chunkæ–‡ä»¶
    deleted_size = 0
    for chunk_file in chunk_files:
        chunk_path = os.path.join(chunk_dir, chunk_file)
        deleted_size += os.path.getsize(chunk_path)
        os.remove(chunk_path)
    
    print(f"âœ… å·²åˆ é™¤ {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
    print(f"ğŸ’¾ é‡Šæ”¾ç©ºé—´: {deleted_size / (1024**3):.2f} GB")

def main():
    """ä¸»å‡½æ•° - åˆå¹¶å·¥å…·çš„å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ•°æ®åˆå¹¶å·¥å…·")
    parser.add_argument("--method", choices=["simple", "optimized", "large"], 
                       default="optimized", help="åˆå¹¶æ–¹æ³•")
    parser.add_argument("--dataset", choices=["train", "valid", "test", "all"],
                       default="all", help="è¦åˆå¹¶çš„æ•°æ®é›†")
    parser.add_argument("--workers", type=int, default=2, help="å·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--verify", action="store_true", help="éªŒè¯åˆå¹¶ç»“æœ")
    parser.add_argument("--cleanup", action="store_true", help="æ¸…ç†chunkæ–‡ä»¶")
    
    args = parser.parse_args()
    
    print("ğŸ”§ æ•°æ®åˆå¹¶å·¥å…·")
    print("=" * 40)
    
    # é€‰æ‹©æ•°æ®é›†
    if args.dataset == "all":
        datasets = ["valid", "test", "train"]
    else:
        datasets = [args.dataset]
    
    # é€‰æ‹©åˆå¹¶æ–¹æ³•
    merge_func = {
        "simple": merge_chunks_simple,
        "optimized": lambda dt: merge_chunks_optimized(dt, args.workers),
        "large": merge_chunks_large_files
    }[args.method]
    
    print(f"ğŸ”§ ä½¿ç”¨æ–¹æ³•: {args.method}")
    print(f"ğŸ“Š æ•°æ®é›†: {datasets}")
    
    # æ‰§è¡Œåˆå¹¶
    success_count = 0
    for dataset in datasets:
        print(f"\n{'='*20} {dataset.upper()} {'='*20}")
        
        if merge_func(dataset):
            success_count += 1
            print(f"âœ… {dataset} åˆå¹¶æˆåŠŸ")
            
            # æ¸…ç†chunkæ–‡ä»¶
            if args.cleanup:
                cleanup_chunk_files(dataset, confirm=False)
        else:
            print(f"âŒ {dataset} åˆå¹¶å¤±è´¥")
    
    # éªŒè¯ç»“æœ
    if args.verify:
        verify_merged_files()
    
    print(f"\nğŸ¯ åˆå¹¶å®Œæˆ: {success_count}/{len(datasets)} ä¸ªæ•°æ®é›†")

if __name__ == "__main__":
    main()
