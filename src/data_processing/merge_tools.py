#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åˆå¹¶å·¥å…·é›†åˆ

åŒ…å«å„ç§åˆå¹¶chunkæ–‡ä»¶çš„å·¥å…·å’Œæ–¹æ³•ï¼š
1. ä¼˜åŒ–ç‰ˆåˆå¹¶ - å†…å­˜å‹å¥½çš„æµå¼åˆå¹¶
2. ç®€å•åˆå¹¶ - åŸºç¡€çš„ä¸€æ¬¡æ€§åˆå¹¶
3. å¤§æ–‡ä»¶åˆå¹¶ - ä¸“é—¨å¤„ç†å¤§chunkæ–‡ä»¶
4. éªŒè¯å·¥å…· - æ£€æŸ¥åˆå¹¶ç»“æœ
"""

import os
import torch
import sys
import json
import gc
import time

from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
import config

def get_chunk_files(data_type):
    """è·å–æŒ‡å®šæ•°æ®ç±»å‹çš„æ‰€æœ‰chunkæ–‡ä»¶"""
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    if not os.path.exists(chunk_dir):
        return []
    
    chunk_files = []
    for file in os.listdir(chunk_dir):
        if file.startswith("chunk_") and file.endswith(".pt"):
            try:
                chunk_num = int(file.split('_')[1].split('.')[0])
                chunk_files.append((chunk_num, file))
            except:
                pass
    
    return sorted(chunk_files, key=lambda x: x[0])

def merge_chunks_simple(data_type):
    """ç®€å•åˆå¹¶ - ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰chunk"""
    print(f"ğŸ”„ ç®€å•åˆå¹¶ {data_type}...")
    
    chunk_files = get_chunk_files(data_type)
    if not chunk_files:
        print(f"âŒ æœªæ‰¾åˆ°chunkæ–‡ä»¶")
        return False
    
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
    
    print(f"ğŸ“¦ æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    if os.path.exists(output_file):
        response = input(f"âš ï¸  {output_file} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–? (y/n): ")
        if response.lower() != 'y':
            return False
    
    # åˆå¹¶æ•°æ®
    all_data = []
    total_dialogues = 0
    
    for chunk_num, chunk_file in tqdm(chunk_files, desc=f"åˆå¹¶{data_type}"):
        chunk_path = os.path.join(chunk_dir, chunk_file)
        try:
            data = torch.load(chunk_path, weights_only=True)
            all_data.extend(data)
            total_dialogues += len(data)
            del data
            gc.collect()
        except Exception as e:
            print(f"âŒ åŠ è½½ {chunk_file} å¤±è´¥: {e}")
            return False
    
    # ä¿å­˜ç»“æœ
    print(f"ğŸ’¾ ä¿å­˜åˆ° {output_file}...")
    torch.save(all_data, output_file)
    
    # éªŒè¯
    file_size = os.path.getsize(output_file) / (1024**3)
    print(f"âœ… {data_type}.pt ä¿å­˜æˆåŠŸ")
    print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {total_dialogues:,}")
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} GB")
    
    return True

def merge_chunks_optimized(data_type, max_workers=2, batch_size=10):
    """ä¼˜åŒ–ç‰ˆåˆå¹¶ - æµå¼å¤„ç†ï¼Œå†…å­˜å‹å¥½"""
    print(f"ğŸ”„ ä¼˜åŒ–åˆå¹¶ {data_type} (æµå¼å¤„ç†)...")

    chunk_files = get_chunk_files(data_type)
    if not chunk_files:
        print(f"âŒ æœªæ‰¾åˆ°chunkæ–‡ä»¶")
        return False

    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
    temp_file = output_file + ".tmp"

    print(f"ğŸ“¦ æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")

    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    if os.path.exists(output_file):
        response = input(f"âš ï¸  {output_file} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–? (y/n): ")
        if response.lower() != 'y':
            return False

    # åˆå§‹åŒ–ä¸´æ—¶æ–‡ä»¶
    torch.save([], temp_file)
    total_dialogues = 0

    def load_chunk_worker(chunk_info):
        """å·¥ä½œçº¿ç¨‹ï¼šåŠ è½½chunk"""
        chunk_num, chunk_file = chunk_info
        chunk_path = os.path.join(chunk_dir, chunk_file)
        try:
            data = torch.load(chunk_path, weights_only=True)
            return chunk_num, data, len(data)
        except Exception as e:
            print(f"âŒ åŠ è½½ {chunk_file} å¤±è´¥: {e}")
            return chunk_num, None, 0

    def append_to_temp_file(new_data):
        """è¿½åŠ æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶"""
        existing_data = torch.load(temp_file, weights_only=True)
        existing_data.extend(new_data)
        torch.save(existing_data, temp_file)
        del existing_data
        gc.collect()

    # åˆ†æ‰¹å¤„ç†
    with tqdm(total=len(chunk_files), desc=f"åˆå¹¶{data_type}") as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            for i in range(0, len(chunk_files), batch_size):
                batch = chunk_files[i:i+batch_size]

                # æäº¤åŠ è½½ä»»åŠ¡
                futures = [executor.submit(load_chunk_worker, chunk_info) for chunk_info in batch]

                # æ”¶é›†ç»“æœ
                batch_data = []
                for future in futures:
                    chunk_num, data, count = future.result()
                    if data is not None:
                        batch_data.extend(data)
                        total_dialogues += count
                        del data

                # è¿½åŠ åˆ°æ–‡ä»¶
                if batch_data:
                    append_to_temp_file(batch_data)
                    del batch_data
                    gc.collect()

                pbar.update(len(batch))

    # å®Œæˆåˆå¹¶
    if os.path.exists(output_file):
        os.remove(output_file)
    os.rename(temp_file, output_file)

    # éªŒè¯
    file_size = os.path.getsize(output_file) / (1024**3)
    print(f"âœ… {data_type}.pt ä¿å­˜æˆåŠŸ")
    print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {total_dialogues:,}")
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} GB")

    return True

def merge_chunks_ultra_optimized(data_type, max_workers=2, batch_size=20, save_interval=50):
    """è¶…çº§ä¼˜åŒ–ç‰ˆåˆå¹¶ - åˆ†æ®µä¿å­˜ï¼ŒçœŸæ­£çš„æµå¼å¤„ç†"""
    print(f"ğŸš€ è¶…çº§ä¼˜åŒ–åˆå¹¶ {data_type} (åˆ†æ®µæµå¼å¤„ç†)...")

    chunk_files = get_chunk_files(data_type)
    if not chunk_files:
        print(f"âŒ æœªæ‰¾åˆ°chunkæ–‡ä»¶")
        return False

    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
    temp_dir = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}_temp_segments")
    resume_file = os.path.join(chunk_dir, f"resume_{data_type}.json")

    print(f"ğŸ“¦ æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")

    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    if os.path.exists(output_file):
        response = input(f"âš ï¸  {output_file} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–? (y/n): ")
        if response.lower() != 'y':
            return False

    # åˆ›å»ºä¸´æ—¶ç›®å½•
    os.makedirs(temp_dir, exist_ok=True)

    # åŠ è½½æ–­ç‚¹ç»­ä¼ çŠ¶æ€
    start_chunk = 0
    total_dialogues = 0
    if os.path.exists(resume_file):
        try:
            with open(resume_file, 'r') as f:
                resume_data = json.load(f)
            start_chunk = resume_data.get("processed_chunks", 0)
            total_dialogues = resume_data.get("total_dialogues", 0)
            print(f"ğŸ”„ ä»ç¬¬ {start_chunk} ä¸ªchunkç»§ç»­ (å·²å¤„ç† {total_dialogues:,} ä¸ªå¯¹è¯)")
        except:
            print("âš ï¸  resumeæ–‡ä»¶æŸåï¼Œä»å¤´å¼€å§‹")

    def load_chunk_worker(chunk_info):
        """å·¥ä½œçº¿ç¨‹ï¼šåŠ è½½chunk"""
        chunk_num, chunk_file = chunk_info
        chunk_path = os.path.join(chunk_dir, chunk_file)
        try:
            data = torch.load(chunk_path, weights_only=True)
            return chunk_num, data, len(data)
        except Exception as e:
            print(f"âŒ åŠ è½½ {chunk_file} å¤±è´¥: {e}")
            return chunk_num, None, 0

    def save_resume_state(processed_chunks, total_dialogues):
        """ä¿å­˜æ–­ç‚¹ç»­ä¼ çŠ¶æ€"""
        state = {
            "processed_chunks": processed_chunks,
            "total_dialogues": total_dialogues,
            "timestamp": time.time()
        }
        with open(resume_file, 'w') as f:
            json.dump(state, f)

    # åˆ†æ®µå¤„ç†
    segment_count = 0
    accumulated_data = []
    processed_chunks = start_chunk

    print(f"ğŸ“Š å¤„ç†å‚æ•°: batch_size={batch_size}, save_interval={save_interval}, workers={max_workers}")

    with tqdm(total=len(chunk_files), initial=start_chunk, desc=f"åˆå¹¶{data_type}") as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            for i in range(start_chunk, len(chunk_files), batch_size):
                batch = chunk_files[i:i+batch_size]

                # æäº¤åŠ è½½ä»»åŠ¡
                futures = [executor.submit(load_chunk_worker, chunk_info) for chunk_info in batch]

                # æ”¶é›†ç»“æœ
                batch_data = []
                for future in futures:
                    chunk_num, data, count = future.result()
                    if data is not None:
                        batch_data.extend(data)
                        total_dialogues += count
                        del data

                # ç´¯ç§¯æ•°æ®
                if batch_data:
                    accumulated_data.extend(batch_data)
                    del batch_data
                    gc.collect()

                processed_chunks += len(batch)
                pbar.update(len(batch))

                # å®šæœŸä¿å­˜åˆ†æ®µæ–‡ä»¶
                if len(accumulated_data) >= save_interval * 20000 or processed_chunks >= len(chunk_files):
                    if accumulated_data:
                        segment_file = os.path.join(temp_dir, f"segment_{segment_count:04d}.pt")
                        torch.save(accumulated_data, segment_file)
                        print(f"ğŸ’¾ ä¿å­˜åˆ†æ®µ {segment_count}: {len(accumulated_data):,} ä¸ªå¯¹è¯")

                        del accumulated_data
                        accumulated_data = []
                        segment_count += 1
                        gc.collect()

                # ä¿å­˜æ–­ç‚¹ç»­ä¼ çŠ¶æ€
                save_resume_state(processed_chunks, total_dialogues)

    # åˆå¹¶æ‰€æœ‰åˆ†æ®µæ–‡ä»¶
    print(f"\nğŸ”— åˆå¹¶ {segment_count} ä¸ªåˆ†æ®µæ–‡ä»¶...")
    final_data = []

    for i in range(segment_count):
        segment_file = os.path.join(temp_dir, f"segment_{i:04d}.pt")
        if os.path.exists(segment_file):
            segment_data = torch.load(segment_file, weights_only=True)
            final_data.extend(segment_data)
            del segment_data
            gc.collect()
            print(f"âœ… åˆå¹¶åˆ†æ®µ {i+1}/{segment_count}")

    # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
    print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ–‡ä»¶...")
    torch.save(final_data, output_file)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    import shutil
    shutil.rmtree(temp_dir)
    if os.path.exists(resume_file):
        os.remove(resume_file)

    # éªŒè¯
    file_size = os.path.getsize(output_file) / (1024**3)
    print(f"âœ… {data_type}.pt ä¿å­˜æˆåŠŸ")
    print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {total_dialogues:,}")
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} GB")

    return True

def merge_chunks_large_files(data_type, timeout_seconds=300):
    """å¤§æ–‡ä»¶åˆå¹¶ - ä¸“é—¨å¤„ç†å¤§chunkæ–‡ä»¶"""
    print(f"ğŸ”„ å¤§æ–‡ä»¶åˆå¹¶ {data_type}...")
    
    chunk_files = get_chunk_files(data_type)
    if not chunk_files:
        print(f"âŒ æœªæ‰¾åˆ°chunkæ–‡ä»¶")
        return False
    
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
    
    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    total_size = 0
    for chunk_num, chunk_file in chunk_files:
        chunk_path = os.path.join(chunk_dir, chunk_file)
        size = os.path.getsize(chunk_path)
        size_mb = size / (1024*1024)
        total_size += size
        print(f"   ğŸ“„ {chunk_file}: {size_mb:.1f} MB")
    
    print(f"ğŸ“Š æ€»å¤§å°: {total_size / (1024*1024*1024):.2f} GB")
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    if os.path.exists(output_file):
        response = input(f"âš ï¸  {output_file} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–? (y/n): ")
        if response.lower() != 'y':
            return False
    
    # é€ä¸ªå¤„ç†å¤§æ–‡ä»¶
    all_data = []
    total_dialogues = 0
    
    for i, (chunk_num, chunk_file) in enumerate(chunk_files):
        chunk_path = os.path.join(chunk_dir, chunk_file)
        size_mb = os.path.getsize(chunk_path) / (1024*1024)
        
        print(f"\nğŸ“¦ å¤„ç† {chunk_file} ({i+1}/{len(chunk_files)})...")
        print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")
        
        start_time = time.time()
        
        try:
            print(f"   ğŸ”„ å¼€å§‹åŠ è½½... (è¶…æ—¶: {timeout_seconds}ç§’)")
            data = torch.load(chunk_path, weights_only=True)
            
            load_time = time.time() - start_time
            dialogue_count = len(data)
            
            print(f"   âœ… åŠ è½½æˆåŠŸ!")
            print(f"   â±ï¸  åŠ è½½æ—¶é—´: {load_time:.1f}ç§’")
            print(f"   ğŸ“Š å¯¹è¯æ•°: {dialogue_count:,}")
            
            # åˆå¹¶æ•°æ®
            all_data.extend(data)
            total_dialogues += dialogue_count
            
            print(f"   ğŸ“Š ç´¯è®¡å¯¹è¯æ•°: {total_dialogues:,}")
            
            # æ¸…ç†å†…å­˜
            del data
            gc.collect()
            
        except Exception as e:
            load_time = time.time() - start_time
            print(f"   âŒ åŠ è½½å¤±è´¥ (ç”¨æ—¶ {load_time:.1f}ç§’): {e}")
            
            response = input(f"   æ˜¯å¦è·³è¿‡æ­¤æ–‡ä»¶ç»§ç»­? (y/n): ")
            if response.lower() != 'y':
                return False
            continue
    
    # ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœ...")
    save_start = time.time()
    torch.save(all_data, output_file)
    save_time = time.time() - save_start
    
    print(f"âœ… ä¿å­˜æˆåŠŸ!")
    print(f"â±ï¸  ä¿å­˜æ—¶é—´: {save_time:.1f}ç§’")
    
    # éªŒè¯
    file_size = os.path.getsize(output_file) / (1024**3)
    print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶å¤§å°: {file_size:.2f} GB")
    print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {total_dialogues:,}")
    
    return True

def verify_merged_files():
    """éªŒè¯åˆå¹¶åçš„æ–‡ä»¶"""
    print("ğŸ” éªŒè¯åˆå¹¶åçš„æ–‡ä»¶...")
    
    datasets = ["train", "valid", "test"]
    
    for dataset in datasets:
        output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{dataset}.pt")
        
        if os.path.exists(output_file):
            try:
                print(f"\nğŸ“„ éªŒè¯ {dataset}.pt...")
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(output_file) / (1024**3)
                print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} GB")
                
                # åŠ è½½å¹¶æ£€æŸ¥æ•°æ®
                data = torch.load(output_file, weights_only=True)
                print(f"   ğŸ“Š å¯¹è¯æ•°é‡: {len(data):,}")
                
                # æ£€æŸ¥æ•°æ®ç»“æ„
                if data:
                    sample = data[0]
                    if isinstance(sample, tuple) and len(sample) == 2:
                        x_ref, steps = sample
                        print(f"   âœ… æ•°æ®ç»“æ„æ­£ç¡®")
                        print(f"   ğŸ“Š æ ·æœ¬: x_ref={x_ref.shape}, steps={len(steps)}")
                    else:
                        print(f"   âš ï¸  æ•°æ®ç»“æ„å¼‚å¸¸: {type(sample)}")
                
                del data
                gc.collect()
                
                print(f"   âœ… {dataset}.pt éªŒè¯é€šè¿‡")
                
            except Exception as e:
                print(f"   âŒ {dataset}.pt éªŒè¯å¤±è´¥: {e}")
        else:
            print(f"   âŒ {dataset}.pt ä¸å­˜åœ¨")

def cleanup_chunk_files(data_type, confirm=True):
    """æ¸…ç†chunkæ–‡ä»¶ï¼ˆåœ¨åˆå¹¶å®Œæˆåï¼‰"""
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    if not os.path.exists(chunk_dir):
        return
    
    chunk_files = [f for f in os.listdir(chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]
    
    if not chunk_files:
        print(f"ğŸ“ {data_type}: æ— chunkæ–‡ä»¶éœ€è¦æ¸…ç†")
        return
    
    print(f"ğŸ“ {data_type}: æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
    
    if confirm:
        response = input(f"æ˜¯å¦åˆ é™¤ {data_type} çš„chunkæ–‡ä»¶é‡Šæ”¾ç©ºé—´? (y/n): ")
        if response.lower() != 'y':
            return
    
    # åˆ é™¤chunkæ–‡ä»¶
    deleted_size = 0
    for chunk_file in chunk_files:
        chunk_path = os.path.join(chunk_dir, chunk_file)
        deleted_size += os.path.getsize(chunk_path)
        os.remove(chunk_path)
    
    print(f"âœ… å·²åˆ é™¤ {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
    print(f"ğŸ’¾ é‡Šæ”¾ç©ºé—´: {deleted_size / (1024**3):.2f} GB")

def main():
    """ä¸»å‡½æ•° - åˆå¹¶å·¥å…·çš„å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ•°æ®åˆå¹¶å·¥å…·")
    parser.add_argument("--method", choices=["simple", "optimized", "large", "ultra"],
                       default="optimized", help="åˆå¹¶æ–¹æ³•")
    parser.add_argument("--dataset", choices=["train", "valid", "test", "all"],
                       default="all", help="è¦åˆå¹¶çš„æ•°æ®é›†")
    parser.add_argument("--workers", type=int, default=2, help="å·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--batch-size", type=int, default=20, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--save-interval", type=int, default=50, help="ä¿å­˜é—´éš”(ä»…ultraæ–¹æ³•)")
    parser.add_argument("--verify", action="store_true", help="éªŒè¯åˆå¹¶ç»“æœ")
    parser.add_argument("--cleanup", action="store_true", help="æ¸…ç†chunkæ–‡ä»¶")
    
    args = parser.parse_args()
    
    print("ğŸ”§ æ•°æ®åˆå¹¶å·¥å…·")
    print("=" * 40)
    
    # é€‰æ‹©æ•°æ®é›†
    if args.dataset == "all":
        datasets = ["valid", "test", "train"]
    else:
        datasets = [args.dataset]
    
    # é€‰æ‹©åˆå¹¶æ–¹æ³•
    merge_func = {
        "simple": merge_chunks_simple,
        "optimized": lambda dt: merge_chunks_optimized(dt, args.workers, args.batch_size),
        "large": merge_chunks_large_files,
        "ultra": lambda dt: merge_chunks_ultra_optimized(dt, args.workers, args.batch_size, args.save_interval)
    }[args.method]

    print(f"ğŸ”§ ä½¿ç”¨æ–¹æ³•: {args.method}")
    print(f"ğŸ“Š æ•°æ®é›†: {datasets}")
    if args.method in ["optimized", "ultra"]:
        print(f"âš™ï¸  å‚æ•°: workers={args.workers}, batch_size={args.batch_size}")
        if args.method == "ultra":
            print(f"âš™ï¸  ä¿å­˜é—´éš”: {args.save_interval}")
    
    # æ‰§è¡Œåˆå¹¶
    success_count = 0
    for dataset in datasets:
        print(f"\n{'='*20} {dataset.upper()} {'='*20}")
        
        if merge_func(dataset):
            success_count += 1
            print(f"âœ… {dataset} åˆå¹¶æˆåŠŸ")
            
            # æ¸…ç†chunkæ–‡ä»¶
            if args.cleanup:
                cleanup_chunk_files(dataset, confirm=False)
        else:
            print(f"âŒ {dataset} åˆå¹¶å¤±è´¥")
    
    # éªŒè¯ç»“æœ
    if args.verify:
        verify_merged_files()
    
    print(f"\nğŸ¯ åˆå¹¶å®Œæˆ: {success_count}/{len(datasets)} ä¸ªæ•°æ®é›†")

if __name__ == "__main__":
    main()
