#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å¤„ç†å·¥å…·å‡½æ•°

åŒ…å«å„ç§æ•°æ®å¤„ç†ç›¸å…³çš„å·¥å…·å‡½æ•°ï¼š
1. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
2. å¤„ç†æ—¶é—´ä¼°ç®—
3. ç£ç›˜ç©ºé—´ç®¡ç†
4. æ–‡ä»¶æ“ä½œå·¥å…·
"""

import os
import json
import torch
import glob
import shutil
import sys
from typing import Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
import config


# ç»Ÿä¸€ç§»é™¤è¾“å‡ºä¸­çš„emojiï¼Œé¿å…æ§åˆ¶å°/æ—¥å¿—ç¯å¢ƒå…¼å®¹é—®é¢˜
import re, builtins as _builtins
_EMOJI_RE = re.compile(r"[\U0001F300-\U0001FAFF\U00002700-\U000027BF\U00002600-\U000026FF\U00002190-\U00002BFF]")
_print = _builtins.print

def print(*args, **kwargs):
    def _strip(s):
        return _EMOJI_RE.sub('', s) if isinstance(s, str) else s
    return _print(*[_strip(a) for a in args], **kwargs)

def check_data_integrity(data_type: str = None) -> Dict:
    """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
    print("ğŸ” æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")

    if data_type:
        data_types = [data_type]
    else:
        data_types = ["train", "valid", "test"]

    results = {}

    for dtype in data_types:
        print(f"\nğŸ“ æ£€æŸ¥ {dtype}:")

        result = {
            "original_file": None,
            "chunk_files": [],
            "merged_file": None,
            "partial_files": [],
            "resume_file": None
        }

        # æ£€æŸ¥åŸå§‹æ–‡ä»¶
        original_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{dtype}.jsonl")
        if os.path.exists(original_file):
            size_mb = os.path.getsize(original_file) / (1024*1024)
            result["original_file"] = {
                "exists": True,
                "size_mb": size_mb,
                "path": original_file
            }
            print(f"   ğŸ“„ åŸå§‹æ–‡ä»¶: {size_mb:.1f} MB")
        else:
            result["original_file"] = {"exists": False}
            print(f"   ğŸ“„ åŸå§‹æ–‡ä»¶: ä¸å­˜åœ¨")

        # æ£€æŸ¥chunkç›®å½•
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, dtype)
        if os.path.exists(chunk_dir):
            # chunkæ–‡ä»¶
            chunk_files = [f for f in os.listdir(chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]
            chunk_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))

            for chunk_file in chunk_files:
                chunk_path = os.path.join(chunk_dir, chunk_file)
                size_mb = os.path.getsize(chunk_path) / (1024*1024)
                chunk_num = int(chunk_file.split('_')[1].split('.')[0])
                result["chunk_files"].append({
                    "name": chunk_file,
                    "number": chunk_num,
                    "size_mb": size_mb,
                    "path": chunk_path
                })

            print(f"   ğŸ“¦ chunkæ–‡ä»¶: {len(chunk_files)} ä¸ª")

            # partialæ–‡ä»¶
            partial_files = [f for f in os.listdir(chunk_dir) if f.startswith("partial_") and f.endswith(".pt")]
            for partial_file in partial_files:
                partial_path = os.path.join(chunk_dir, partial_file)
                size_mb = os.path.getsize(partial_path) / (1024*1024)
                result["partial_files"].append({
                    "name": partial_file,
                    "size_mb": size_mb,
                    "path": partial_path
                })

            if partial_files:
                print(f"   ğŸ”„ partialæ–‡ä»¶: {len(partial_files)} ä¸ª")

            # resumeæ–‡ä»¶
            resume_file = os.path.join(chunk_dir, f"resume_{dtype}.json")
            if os.path.exists(resume_file):
                try:
                    with open(resume_file, 'r') as f:
                        resume_data = json.load(f)
                    result["resume_file"] = resume_data
                    print(f"   ğŸ“‹ resumeæ–‡ä»¶: {resume_data}")
                except:
                    result["resume_file"] = {"error": "damaged"}
                    print(f"   ğŸ“‹ resumeæ–‡ä»¶: æŸå")

        # æ£€æŸ¥åˆå¹¶æ–‡ä»¶
        merged_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{dtype}.pt")
        if os.path.exists(merged_file):
            size_gb = os.path.getsize(merged_file) / (1024**3)
            try:
                data = torch.load(merged_file, weights_only=True)
                dialogue_count = len(data)
                del data
                result["merged_file"] = {
                    "exists": True,
                    "size_gb": size_gb,
                    "dialogue_count": dialogue_count,
                    "path": merged_file
                }
                print(f"   ğŸ“„ åˆå¹¶æ–‡ä»¶: {size_gb:.2f} GB, {dialogue_count:,} ä¸ªå¯¹è¯")
            except:
                result["merged_file"] = {
                    "exists": True,
                    "size_gb": size_gb,
                    "error": "cannot_load"
                }
                print(f"   ğŸ“„ åˆå¹¶æ–‡ä»¶: {size_gb:.2f} GB (æ— æ³•åŠ è½½)")
        else:
            result["merged_file"] = {"exists": False}
            print(f"   ğŸ“„ åˆå¹¶æ–‡ä»¶: ä¸å­˜åœ¨")

        results[dtype] = result

    return results

def estimate_processing_time(data_type: str = None) -> Dict:
    """ä¼°ç®—å¤„ç†æ—¶é—´"""
    print("â±ï¸  ä¼°ç®—å¤„ç†æ—¶é—´...")

    if data_type:
        data_types = [data_type]
    else:
        data_types = ["train", "valid", "test"]

    estimates = {}

    for dtype in data_types:
        print(f"\nğŸ“Š ä¼°ç®— {dtype}:")

        # æ£€æŸ¥åŸå§‹æ–‡ä»¶
        original_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{dtype}.jsonl")
        if not os.path.exists(original_file):
            print(f"   âŒ åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨")
            continue

        # æ–‡ä»¶å¤§å°
        file_size_mb = os.path.getsize(original_file) / (1024*1024)
        print(f"   ğŸ“„ æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")

        # ä¼°ç®—è¡Œæ•°
        with open(original_file, 'r', encoding='utf-8') as f:
            sample_lines = []
            for i, line in enumerate(f):
                if i >= 100:
                    break
                sample_lines.append(len(line.encode('utf-8')))

        if sample_lines:
            avg_line_size = sum(sample_lines) / len(sample_lines)
            estimated_lines = int(file_size_mb * 1024 * 1024 / avg_line_size)
            print(f"   ğŸ“Š ä¼°ç®—è¡Œæ•°: {estimated_lines:,}")

            # å¤„ç†æ—¶é—´ä¼°ç®—
            speeds = {
                "å•çº¿ç¨‹": 100,
                "2çº¿ç¨‹": 180,
                "4çº¿ç¨‹": 300,
                "8çº¿ç¨‹": 400
            }

            time_estimates = {}
            for mode, speed in speeds.items():
                time_seconds = estimated_lines / speed
                time_minutes = time_seconds / 60
                time_hours = time_minutes / 60

                if time_hours > 1:
                    time_str = f"{time_hours:.1f} å°æ—¶"
                elif time_minutes > 1:
                    time_str = f"{time_minutes:.1f} åˆ†é’Ÿ"
                else:
                    time_str = f"{time_seconds:.1f} ç§’"

                time_estimates[mode] = {
                    "seconds": time_seconds,
                    "display": time_str
                }
                print(f"   {mode}: {time_str}")

            estimates[dtype] = {
                "file_size_mb": file_size_mb,
                "estimated_lines": estimated_lines,
                "time_estimates": time_estimates
            }

    return estimates

def get_disk_usage() -> Dict:
    """è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µ"""
    try:
        total, used, free = shutil.disk_usage(config.LCCC_PROCESSED_PATH)
        return {
            "total_gb": total / (1024**3),
            "used_gb": used / (1024**3),
            "free_gb": free / (1024**3),
            "usage_percent": (used / total) * 100
        }
    except Exception as e:
        return {"error": str(e)}

def estimate_space_requirements() -> Dict:
    """ä¼°ç®—ç©ºé—´éœ€æ±‚"""
    print("ğŸ’¾ ä¼°ç®—ç©ºé—´éœ€æ±‚...")

    space_info = get_disk_usage()
    if "error" in space_info:
        print(f"âŒ æ— æ³•è·å–ç£ç›˜ä¿¡æ¯: {space_info['error']}")
        return space_info

    print(f"ğŸ’¿ å½“å‰ç£ç›˜çŠ¶æ€:")
    print(f"   æ€»ç©ºé—´: {space_info['total_gb']:.1f} GB")
    print(f"   å·²ä½¿ç”¨: {space_info['used_gb']:.1f} GB")
    print(f"   å¯ç”¨ç©ºé—´: {space_info['free_gb']:.1f} GB")
    print(f"   ä½¿ç”¨ç‡: {space_info['usage_percent']:.1f}%")

    # ä¼°ç®—å„ç±»æ–‡ä»¶çš„ç©ºé—´å ç”¨
    file_sizes = {}

    # chunkæ–‡ä»¶
    chunk_size = 0
    for data_type in ["train", "valid", "test"]:
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
        if os.path.exists(chunk_dir):
            for file in os.listdir(chunk_dir):
                if file.startswith("chunk_") and file.endswith(".pt"):
                    file_path = os.path.join(chunk_dir, file)
                    chunk_size += os.path.getsize(file_path)

    file_sizes["chunks_gb"] = chunk_size / (1024**3)

    # åŸå§‹æ–‡ä»¶
    original_size = 0
    for data_type in ["train", "valid", "test"]:
        original_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.jsonl")
        if os.path.exists(original_file):
            original_size += os.path.getsize(original_file)

    file_sizes["original_gb"] = original_size / (1024**3)

    # åˆå¹¶æ–‡ä»¶
    merged_size = 0
    for data_type in ["train", "valid", "test"]:
        merged_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
        if os.path.exists(merged_file):
            merged_size += os.path.getsize(merged_file)

    file_sizes["merged_gb"] = merged_size / (1024**3)

    print(f"\nğŸ“Š æ–‡ä»¶ç©ºé—´å ç”¨:")
    print(f"   åŸå§‹æ–‡ä»¶: {file_sizes['original_gb']:.1f} GB")
    print(f"   chunkæ–‡ä»¶: {file_sizes['chunks_gb']:.1f} GB")
    print(f"   åˆå¹¶æ–‡ä»¶: {file_sizes['merged_gb']:.1f} GB")

    # ä¼°ç®—åˆå¹¶æ‰€éœ€ç©ºé—´
    estimated_merged_size = file_sizes["chunks_gb"] * 0.8  # ä¼°ç®—åˆå¹¶åå¤§å°
    required_space = estimated_merged_size * 1.2  # 20%ç¼“å†²

    print(f"\nğŸ’¡ ç©ºé—´å»ºè®®:")
    print(f"   ä¼°ç®—åˆå¹¶åå¤§å°: {estimated_merged_size:.1f} GB")
    print(f"   å»ºè®®å¯ç”¨ç©ºé—´: {required_space:.1f} GB")

    if space_info["free_gb"] >= required_space:
        print(f"   âœ… ç©ºé—´å……è¶³")
    else:
        print(f"   âš ï¸  ç©ºé—´å¯èƒ½ä¸è¶³")
        print(f"   éœ€è¦é‡Šæ”¾: {required_space - space_info['free_gb']:.1f} GB")

    space_info.update(file_sizes)
    space_info["estimated_merged_gb"] = estimated_merged_size
    space_info["required_space_gb"] = required_space

    return space_info

def cleanup_partial_files(data_type: str = None, confirm: bool = True) -> int:
    """æ¸…ç†partialæ–‡ä»¶"""
    if data_type:
        data_types = [data_type]
    else:
        data_types = ["train", "valid", "test"]

    total_cleaned = 0

    for dtype in data_types:
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, dtype)
        if not os.path.exists(chunk_dir):
            continue

        partial_files = [f for f in os.listdir(chunk_dir) if f.startswith("partial_") and f.endswith(".pt")]

        if not partial_files:
            continue

        print(f"ğŸ“ {dtype}: æ‰¾åˆ° {len(partial_files)} ä¸ªpartialæ–‡ä»¶")

        if confirm:
            response = input(f"æ˜¯å¦åˆ é™¤ {dtype} çš„partialæ–‡ä»¶? (y/n): ")
            if response.lower() != 'y':
                continue

        # åˆ é™¤partialæ–‡ä»¶
        for partial_file in partial_files:
            partial_path = os.path.join(chunk_dir, partial_file)
            try:
                os.remove(partial_path)
                total_cleaned += 1
            except Exception as e:
                print(f"âŒ åˆ é™¤ {partial_file} å¤±è´¥: {e}")

    if total_cleaned > 0:
        print(f"âœ… æ€»å…±æ¸…ç†äº† {total_cleaned} ä¸ªpartialæ–‡ä»¶")

    return total_cleaned

def count_dialogues_in_chunks(data_type: str) -> int:
    """ç»Ÿè®¡chunkæ–‡ä»¶ä¸­çš„å¯¹è¯æ•°é‡"""
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    if not os.path.exists(chunk_dir):
        return 0

    chunk_files = [f for f in os.listdir(chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]

    if not chunk_files:
        return 0

    # é‡‡æ ·ç»Ÿè®¡
    sample_size = min(5, len(chunk_files))
    sample_files = chunk_files[:sample_size]

    total_in_samples = 0
    for chunk_file in sample_files:
        chunk_path = os.path.join(chunk_dir, chunk_file)
        try:
            data = torch.load(chunk_path, weights_only=True)
            total_in_samples += len(data)
            del data
        except:
            pass

    if sample_size > 0:
        avg_per_chunk = total_in_samples / sample_size
        estimated_total = int(avg_per_chunk * len(chunk_files))
        return estimated_total

    return 0

def main():
    """ä¸»å‡½æ•° - æ•°æ®å·¥å…·çš„å‘½ä»¤è¡Œæ¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description="æ•°æ®å¤„ç†å·¥å…·")
    parser.add_argument("--check", action="store_true", help="æ£€æŸ¥æ•°æ®å®Œæ•´æ€§")
    parser.add_argument("--estimate", action="store_true", help="ä¼°ç®—å¤„ç†æ—¶é—´")
    parser.add_argument("--space", action="store_true", help="æ£€æŸ¥ç£ç›˜ç©ºé—´")
    parser.add_argument("--cleanup", action="store_true", help="æ¸…ç†partialæ–‡ä»¶")
    parser.add_argument("--dataset", choices=["train", "valid", "test"], help="æŒ‡å®šæ•°æ®é›†")

    args = parser.parse_args()

    if not any([args.check, args.estimate, args.space, args.cleanup]):
        # é»˜è®¤æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
        args.check = args.estimate = args.space = True

    print("ğŸ”§ æ•°æ®å¤„ç†å·¥å…·")
    print("=" * 40)

    if args.check:
        check_data_integrity(args.dataset)

    if args.estimate:
        estimate_processing_time(args.dataset)

    if args.space:
        estimate_space_requirements()

    if args.cleanup:
        cleanup_partial_files(args.dataset)

if __name__ == "__main__":
    main()
