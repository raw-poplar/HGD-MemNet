#!/usr/bin/env python3
"""
è°ƒè¯•åˆå¹¶å·¥å…· - ç”¨äºŽè¯Šæ–­åˆå¹¶è¿‡ç¨‹ä¸­çš„é—®é¢˜
"""

import os
import sys
import time
import torch
import gc
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

import config

def get_chunk_files(data_type):
    """èŽ·å–chunkæ–‡ä»¶åˆ—è¡¨"""
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    if not os.path.exists(chunk_dir):
        return []
    
    chunk_files = []
    for file in os.listdir(chunk_dir):
        if file.startswith("chunk_") and file.endswith(".pt"):
            try:
                chunk_num = int(file.split('_')[1].split('.')[0])
                chunk_files.append((chunk_num, file))
            except:
                pass
    
    return sorted(chunk_files, key=lambda x: x[0])

def test_single_chunk_load(data_type, chunk_index=0):
    """æµ‹è¯•åŠ è½½å•ä¸ªchunkæ–‡ä»¶"""
    print(f"ðŸ§ª æµ‹è¯•åŠ è½½å•ä¸ªchunkæ–‡ä»¶...")
    
    chunk_files = get_chunk_files(data_type)
    if not chunk_files:
        print(f"âŒ æœªæ‰¾åˆ°chunkæ–‡ä»¶")
        return False
    
    if chunk_index >= len(chunk_files):
        chunk_index = 0
    
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    chunk_num, chunk_file = chunk_files[chunk_index]
    chunk_path = os.path.join(chunk_dir, chunk_file)
    
    print(f"ðŸ“„ æµ‹è¯•æ–‡ä»¶: {chunk_file}")
    print(f"ðŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(chunk_path) / (1024*1024):.1f} MB")
    
    try:
        print(f"ðŸ”„ å¼€å§‹åŠ è½½...")
        start_time = time.time()
        data = torch.load(chunk_path, weights_only=True)
        load_time = time.time() - start_time
        
        print(f"âœ… åŠ è½½æˆåŠŸ!")
        print(f"â±ï¸  åŠ è½½æ—¶é—´: {load_time:.1f}ç§’")
        print(f"ðŸ“Š æ•°æ®é‡: {len(data):,} ä¸ªå¯¹è¯")
        
        # æ£€æŸ¥æ•°æ®ç»“æž„
        if data:
            sample = data[0]
            print(f"ðŸ“Š æ•°æ®ç»“æž„: {type(sample)}")
            if isinstance(sample, tuple):
                print(f"ðŸ“Š å…ƒç»„é•¿åº¦: {len(sample)}")
        
        del data
        gc.collect()
        return True
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return False

def test_multiple_chunks_load(data_type, num_chunks=5):
    """æµ‹è¯•åŠ è½½å¤šä¸ªchunkæ–‡ä»¶"""
    print(f"ðŸ§ª æµ‹è¯•åŠ è½½å¤šä¸ªchunkæ–‡ä»¶ (å‰{num_chunks}ä¸ª)...")
    
    chunk_files = get_chunk_files(data_type)
    if not chunk_files:
        print(f"âŒ æœªæ‰¾åˆ°chunkæ–‡ä»¶")
        return False
    
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    test_chunks = chunk_files[:min(num_chunks, len(chunk_files))]
    
    total_dialogues = 0
    total_time = 0
    
    for i, (chunk_num, chunk_file) in enumerate(test_chunks):
        chunk_path = os.path.join(chunk_dir, chunk_file)
        
        print(f"\nðŸ“¦ æµ‹è¯• {i+1}/{len(test_chunks)}: {chunk_file}")
        print(f"ðŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(chunk_path) / (1024*1024):.1f} MB")
        
        try:
            start_time = time.time()
            data = torch.load(chunk_path, weights_only=True)
            load_time = time.time() - start_time
            
            total_dialogues += len(data)
            total_time += load_time
            
            print(f"âœ… åŠ è½½æˆåŠŸ! æ—¶é—´: {load_time:.1f}ç§’, æ•°æ®: {len(data):,}")
            
            del data
            gc.collect()
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return False
    
    print(f"\nðŸ“Š æ€»ç»“:")
    print(f"âœ… æˆåŠŸåŠ è½½ {len(test_chunks)} ä¸ªæ–‡ä»¶")
    print(f"ðŸ“Š æ€»å¯¹è¯æ•°: {total_dialogues:,}")
    print(f"â±ï¸  æ€»æ—¶é—´: {total_time:.1f}ç§’")
    print(f"ðŸ“Š å¹³å‡é€Ÿåº¦: {total_dialogues/total_time:.0f} å¯¹è¯/ç§’")
    
    return True

def diagnose_system_resources():
    """è¯Šæ–­ç³»ç»Ÿèµ„æº"""
    print(f"ðŸ” ç³»ç»Ÿèµ„æºè¯Šæ–­...")
    
    try:
        import psutil
        
        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        print(f"ðŸ’¾ å†…å­˜: {memory.total / (1024**3):.1f} GB æ€»é‡")
        print(f"ðŸ’¾ å¯ç”¨: {memory.available / (1024**3):.1f} GB ({memory.percent:.1f}% å·²ä½¿ç”¨)")
        
        # CPUä¿¡æ¯
        cpu_percent = psutil.cpu_percent(interval=1)
        print(f"ðŸ–¥ï¸  CPUä½¿ç”¨çŽ‡: {cpu_percent:.1f}%")
        print(f"ðŸ–¥ï¸  CPUæ ¸å¿ƒæ•°: {psutil.cpu_count()}")
        
        # ç£ç›˜ä¿¡æ¯
        disk = psutil.disk_usage('F:')
        print(f"ðŸ’¿ Fç›˜: {disk.total / (1024**3):.1f} GB æ€»é‡")
        print(f"ðŸ’¿ å¯ç”¨: {disk.free / (1024**3):.1f} GB ({(disk.used/disk.total)*100:.1f}% å·²ä½¿ç”¨)")
        
    except ImportError:
        print("âš ï¸  psutilæœªå®‰è£…ï¼Œæ— æ³•èŽ·å–è¯¦ç»†ç³»ç»Ÿä¿¡æ¯")
        print("ðŸ’¡ å¯ä»¥è¿è¡Œ: pip install psutil")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è°ƒè¯•åˆå¹¶å·¥å…·")
    parser.add_argument("--dataset", choices=["train", "valid", "test"], 
                       default="train", help="è¦æµ‹è¯•çš„æ•°æ®é›†")
    parser.add_argument("--test-single", action="store_true", help="æµ‹è¯•å•ä¸ªæ–‡ä»¶åŠ è½½")
    parser.add_argument("--test-multiple", type=int, default=5, help="æµ‹è¯•å¤šä¸ªæ–‡ä»¶åŠ è½½")
    parser.add_argument("--chunk-index", type=int, default=0, help="æµ‹è¯•çš„chunkç´¢å¼•")
    parser.add_argument("--diagnose", action="store_true", help="è¯Šæ–­ç³»ç»Ÿèµ„æº")
    
    args = parser.parse_args()
    
    print("ðŸ”§ è°ƒè¯•åˆå¹¶å·¥å…·")
    print("=" * 40)
    
    if args.diagnose:
        diagnose_system_resources()
        print()
    
    if args.test_single:
        test_single_chunk_load(args.dataset, args.chunk_index)
        print()
    
    if args.test_multiple > 0:
        test_multiple_chunks_load(args.dataset, args.test_multiple)

if __name__ == "__main__":
    main()
