#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•å·¥å…·é›†åˆ

åŒ…å«å„ç§è°ƒè¯•å’Œæ£€æŸ¥å·¥å…·ï¼š
1. chunkæ–‡ä»¶åŠ è½½æµ‹è¯•
2. å¤„ç†çŠ¶æ€åˆ†æ
3. æ•°æ®å…¼å®¹æ€§æ£€æŸ¥
4. æ€§èƒ½æµ‹è¯•å·¥å…·
"""

import os
import torch
import json
import sys
import traceback
import time
import gc
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
import config
from src.dataset import Vocabulary

def test_chunk_loading(data_type: str = None, max_test: int = 3) -> Dict:
    """æµ‹è¯•chunkæ–‡ä»¶åŠ è½½"""
    print("ğŸ” æµ‹è¯•chunkæ–‡ä»¶åŠ è½½...")
    
    if data_type:
        data_types = [data_type]
    else:
        data_types = ["train", "valid", "test"]
    
    results = {}
    
    for dtype in data_types:
        print(f"\nğŸ“ æµ‹è¯• {dtype}:")
        
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, dtype)
        if not os.path.exists(chunk_dir):
            print(f"   âŒ ç›®å½•ä¸å­˜åœ¨: {chunk_dir}")
            results[dtype] = {"error": "directory_not_found"}
            continue
        
        # æŸ¥æ‰¾chunkæ–‡ä»¶
        chunk_files = [f for f in os.listdir(chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]
        chunk_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))
        
        print(f"   ğŸ“¦ æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
        
        if not chunk_files:
            results[dtype] = {"error": "no_chunks"}
            continue
        
        # æµ‹è¯•å‰å‡ ä¸ªchunk
        test_files = chunk_files[:max_test]
        test_results = []
        
        for chunk_file in test_files:
            chunk_path = os.path.join(chunk_dir, chunk_file)
            chunk_result = {
                "file": chunk_file,
                "path": chunk_path,
                "size_mb": 0,
                "load_time": 0,
                "dialogue_count": 0,
                "success": False,
                "error": None
            }
            
            try:
                # æ–‡ä»¶å¤§å°
                chunk_result["size_mb"] = os.path.getsize(chunk_path) / (1024*1024)
                print(f"   ğŸ” æµ‹è¯• {chunk_file} ({chunk_result['size_mb']:.1f} MB)")
                
                # åŠ è½½æµ‹è¯•
                start_time = time.time()
                data = torch.load(chunk_path, weights_only=True)
                chunk_result["load_time"] = time.time() - start_time
                chunk_result["dialogue_count"] = len(data)
                chunk_result["success"] = True
                
                print(f"      âœ… åŠ è½½æˆåŠŸ: {chunk_result['dialogue_count']} ä¸ªå¯¹è¯, {chunk_result['load_time']:.2f}ç§’")
                
                # æ£€æŸ¥æ•°æ®ç»“æ„
                if data:
                    sample = data[0]
                    if isinstance(sample, tuple) and len(sample) == 2:
                        x_ref, steps = sample
                        print(f"      ğŸ“Š æ•°æ®ç»“æ„: x_ref={x_ref.shape}, steps={len(steps)}")
                        
                        # æ£€æŸ¥stepsç»“æ„
                        if steps:
                            x_t, target, gate = steps[0]
                            x_t_info = f"{x_t.shape}" if x_t is not None else "None"
                            print(f"      ğŸ“Š æ­¥éª¤ç»“æ„: x_t={x_t_info}, target={target.shape}, gate={gate.shape}")
                    else:
                        print(f"      âš ï¸  æ•°æ®ç»“æ„å¼‚å¸¸: {type(sample)}")
                
                del data
                gc.collect()
                
            except Exception as e:
                chunk_result["error"] = str(e)
                print(f"      âŒ åŠ è½½å¤±è´¥: {e}")
                print(f"      ğŸ“‹ è¯¦ç»†é”™è¯¯:")
                traceback.print_exc()
            
            test_results.append(chunk_result)
        
        results[dtype] = {
            "total_chunks": len(chunk_files),
            "tested_chunks": len(test_results),
            "test_results": test_results
        }
    
    return results

def analyze_processing_status() -> Dict:
    """åˆ†æå¤„ç†çŠ¶æ€"""
    print("ğŸ“Š åˆ†æå¤„ç†çŠ¶æ€...")
    
    status = {}
    
    for data_type in ["train", "valid", "test"]:
        print(f"\nğŸ“ åˆ†æ {data_type}:")
        
        type_status = {
            "original_file": None,
            "chunks": {"count": 0, "total_size_gb": 0},
            "partials": {"count": 0, "total_size_gb": 0},
            "merged_file": None,
            "resume_info": None,
            "processing_stage": "unknown"
        }
        
        # æ£€æŸ¥åŸå§‹æ–‡ä»¶
        original_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.jsonl")
        if os.path.exists(original_file):
            size_gb = os.path.getsize(original_file) / (1024**3)
            type_status["original_file"] = {"exists": True, "size_gb": size_gb}
            print(f"   ğŸ“„ åŸå§‹æ–‡ä»¶: {size_gb:.2f} GB")
        else:
            type_status["original_file"] = {"exists": False}
            print(f"   ğŸ“„ åŸå§‹æ–‡ä»¶: ä¸å­˜åœ¨")
        
        # æ£€æŸ¥chunkç›®å½•
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
        if os.path.exists(chunk_dir):
            # chunkæ–‡ä»¶
            chunk_files = [f for f in os.listdir(chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]
            if chunk_files:
                total_size = sum(os.path.getsize(os.path.join(chunk_dir, f)) for f in chunk_files)
                type_status["chunks"] = {
                    "count": len(chunk_files),
                    "total_size_gb": total_size / (1024**3)
                }
                print(f"   ğŸ“¦ chunkæ–‡ä»¶: {len(chunk_files)} ä¸ª, {total_size / (1024**3):.2f} GB")
            
            # partialæ–‡ä»¶
            partial_files = [f for f in os.listdir(chunk_dir) if f.startswith("partial_") and f.endswith(".pt")]
            if partial_files:
                total_size = sum(os.path.getsize(os.path.join(chunk_dir, f)) for f in partial_files)
                type_status["partials"] = {
                    "count": len(partial_files),
                    "total_size_gb": total_size / (1024**3)
                }
                print(f"   ğŸ”„ partialæ–‡ä»¶: {len(partial_files)} ä¸ª, {total_size / (1024**3):.2f} GB")
            
            # resumeæ–‡ä»¶
            resume_file = os.path.join(chunk_dir, f"resume_{data_type}.json")
            if os.path.exists(resume_file):
                try:
                    with open(resume_file, 'r') as f:
                        resume_data = json.load(f)
                    type_status["resume_info"] = resume_data
                    print(f"   ğŸ“‹ resumeä¿¡æ¯: {resume_data}")
                except:
                    type_status["resume_info"] = {"error": "damaged"}
                    print(f"   ğŸ“‹ resumeæ–‡ä»¶æŸå")
        
        # æ£€æŸ¥åˆå¹¶æ–‡ä»¶
        merged_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
        if os.path.exists(merged_file):
            size_gb = os.path.getsize(merged_file) / (1024**3)
            type_status["merged_file"] = {"exists": True, "size_gb": size_gb}
            print(f"   ğŸ“„ åˆå¹¶æ–‡ä»¶: {size_gb:.2f} GB")
        else:
            type_status["merged_file"] = {"exists": False}
        
        # åˆ¤æ–­å¤„ç†é˜¶æ®µ
        if type_status["merged_file"]["exists"]:
            type_status["processing_stage"] = "completed"
        elif type_status["chunks"]["count"] > 0:
            if type_status["partials"]["count"] > 0:
                type_status["processing_stage"] = "processing"
            else:
                type_status["processing_stage"] = "chunks_ready"
        elif type_status["original_file"]["exists"]:
            type_status["processing_stage"] = "ready_to_process"
        else:
            type_status["processing_stage"] = "no_data"
        
        stage_names = {
            "completed": "âœ… å·²å®Œæˆ",
            "chunks_ready": "ğŸ“¦ chunkå·²å°±ç»ªï¼Œå¾…åˆå¹¶",
            "processing": "ğŸ”„ æ­£åœ¨å¤„ç†",
            "ready_to_process": "â³ å¾…å¤„ç†",
            "no_data": "âŒ æ— æ•°æ®"
        }
        
        print(f"   ğŸ¯ å¤„ç†é˜¶æ®µ: {stage_names.get(type_status['processing_stage'], 'æœªçŸ¥')}")
        
        status[data_type] = type_status
    
    return status

def test_data_compatibility() -> bool:
    """æµ‹è¯•æ•°æ®å…¼å®¹æ€§"""
    print("ğŸ” æµ‹è¯•æ•°æ®å…¼å®¹æ€§...")
    
    try:
        # åˆ›å»ºæµ‹è¯•è¯æ±‡è¡¨
        vocab = Vocabulary("test")
        vocab.addSentence("hello world test")
        
        # åˆ›å»ºæµ‹è¯•å¯¹è¯
        test_dialogue = [
            {"text": "hello world"},
            {"text": "how are you"},
            {"text": "I am fine"}
        ]
        
        # æµ‹è¯•å¤„ç†å‡½æ•°
        from .prepare_binary_data import process_dialogue_to_tensors
        result = process_dialogue_to_tensors(test_dialogue, vocab)
        
        if result is None:
            print("âŒ å¤„ç†å‡½æ•°è¿”å›None")
            return False
        
        x_ref, steps_data = result
        print(f"âœ… å¤„ç†æˆåŠŸ: x_ref={x_ref.shape}, steps={len(steps_data)}")
        
        # æ£€æŸ¥æ•°æ®ç»“æ„
        for i, (x_t, target, gate) in enumerate(steps_data):
            x_t_info = f"{x_t.shape}" if x_t is not None else "None"
            print(f"   æ­¥éª¤{i}: x_t={x_t_info}, target={target.shape}, gate={gate.item()}")
        
        # æµ‹è¯•æ‰¹å¤„ç†å…¼å®¹æ€§
        from src.dataset import binary_collate_fn
        batch = [result, result]
        
        try:
            x_ref_batch, steps_batch = binary_collate_fn(batch)
            print(f"âœ… æ‰¹å¤„ç†å…¼å®¹: x_ref={x_ref_batch.shape}, steps={len(steps_batch)}")
            return True
        except Exception as e:
            print(f"âŒ æ‰¹å¤„ç†å¤±è´¥: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def benchmark_chunk_loading(data_type: str = "valid", num_chunks: int = 5) -> Dict:
    """æ€§èƒ½æµ‹è¯•ï¼šchunkåŠ è½½é€Ÿåº¦"""
    print(f"âš¡ æ€§èƒ½æµ‹è¯•: {data_type} chunkåŠ è½½é€Ÿåº¦...")
    
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    if not os.path.exists(chunk_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {chunk_dir}")
        return {"error": "directory_not_found"}
    
    chunk_files = [f for f in os.listdir(chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]
    chunk_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))
    
    if not chunk_files:
        print(f"âŒ æ— chunkæ–‡ä»¶")
        return {"error": "no_chunks"}
    
    test_files = chunk_files[:num_chunks]
    results = []
    
    print(f"ğŸ“Š æµ‹è¯• {len(test_files)} ä¸ªchunkæ–‡ä»¶...")
    
    for chunk_file in test_files:
        chunk_path = os.path.join(chunk_dir, chunk_file)
        
        # æ–‡ä»¶ä¿¡æ¯
        file_size_mb = os.path.getsize(chunk_path) / (1024*1024)
        
        # åŠ è½½æµ‹è¯•
        start_time = time.time()
        try:
            data = torch.load(chunk_path, weights_only=True)
            load_time = time.time() - start_time
            dialogue_count = len(data)
            
            # è®¡ç®—é€Ÿåº¦æŒ‡æ ‡
            mb_per_sec = file_size_mb / load_time
            dialogues_per_sec = dialogue_count / load_time
            
            result = {
                "file": chunk_file,
                "size_mb": file_size_mb,
                "load_time": load_time,
                "dialogue_count": dialogue_count,
                "mb_per_sec": mb_per_sec,
                "dialogues_per_sec": dialogues_per_sec,
                "success": True
            }
            
            print(f"   ğŸ“„ {chunk_file}: {load_time:.2f}s, {mb_per_sec:.1f} MB/s, {dialogues_per_sec:.0f} å¯¹è¯/s")
            
            del data
            gc.collect()
            
        except Exception as e:
            result = {
                "file": chunk_file,
                "size_mb": file_size_mb,
                "error": str(e),
                "success": False
            }
            print(f"   âŒ {chunk_file}: åŠ è½½å¤±è´¥ - {e}")
        
        results.append(result)
    
    # ç»Ÿè®¡
    successful_results = [r for r in results if r["success"]]
    if successful_results:
        avg_load_time = sum(r["load_time"] for r in successful_results) / len(successful_results)
        avg_mb_per_sec = sum(r["mb_per_sec"] for r in successful_results) / len(successful_results)
        avg_dialogues_per_sec = sum(r["dialogues_per_sec"] for r in successful_results) / len(successful_results)
        
        print(f"\nğŸ“Š å¹³å‡æ€§èƒ½:")
        print(f"   åŠ è½½æ—¶é—´: {avg_load_time:.2f}s")
        print(f"   åŠ è½½é€Ÿåº¦: {avg_mb_per_sec:.1f} MB/s")
        print(f"   å¤„ç†é€Ÿåº¦: {avg_dialogues_per_sec:.0f} å¯¹è¯/s")
        
        return {
            "tested_files": len(test_files),
            "successful": len(successful_results),
            "results": results,
            "averages": {
                "load_time": avg_load_time,
                "mb_per_sec": avg_mb_per_sec,
                "dialogues_per_sec": avg_dialogues_per_sec
            }
        }
    else:
        return {
            "tested_files": len(test_files),
            "successful": 0,
            "results": results,
            "error": "no_successful_loads"
        }

def main():
    """ä¸»å‡½æ•° - è°ƒè¯•å·¥å…·çš„å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è°ƒè¯•å·¥å…·")
    parser.add_argument("--test-loading", action="store_true", help="æµ‹è¯•chunkæ–‡ä»¶åŠ è½½")
    parser.add_argument("--analyze-status", action="store_true", help="åˆ†æå¤„ç†çŠ¶æ€")
    parser.add_argument("--test-compatibility", action="store_true", help="æµ‹è¯•æ•°æ®å…¼å®¹æ€§")
    parser.add_argument("--benchmark", action="store_true", help="æ€§èƒ½æµ‹è¯•")
    parser.add_argument("--dataset", choices=["train", "valid", "test"], help="æŒ‡å®šæ•°æ®é›†")
    parser.add_argument("--max-test", type=int, default=3, help="æœ€å¤§æµ‹è¯•æ–‡ä»¶æ•°")
    
    args = parser.parse_args()
    
    if not any([args.test_loading, args.analyze_status, args.test_compatibility, args.benchmark]):
        # é»˜è®¤æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        args.test_loading = args.analyze_status = args.test_compatibility = True
    
    print("ğŸ”§ è°ƒè¯•å·¥å…·")
    print("=" * 40)
    
    if args.test_loading:
        test_chunk_loading(args.dataset, args.max_test)
    
    if args.analyze_status:
        analyze_processing_status()
    
    if args.test_compatibility:
        test_data_compatibility()
    
    if args.benchmark:
        benchmark_chunk_loading(args.dataset or "valid")

if __name__ == "__main__":
    main()
