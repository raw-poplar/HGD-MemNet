#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é—ç•™åˆå¹¶è„šæœ¬é›†åˆ

åŒ…å«ä¹‹å‰åˆ›å»ºçš„å„ç§åˆå¹¶è„šæœ¬ï¼Œä¿ç•™ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆï¼š
1. final_merge - å¤§æ–‡ä»¶ä¼˜åŒ–ç‰ˆ
2. optimized_merge - æµå¼å¤„ç†ç‰ˆ
3. simple_merge - ç®€å•ç‰ˆæœ¬
"""

import os
import torch
import sys
import gc
import time
import threading
import queue
import json
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
import config

def merge_large_chunks_legacy(data_type, timeout_seconds=300):
    """é—ç•™ç‰ˆæœ¬ï¼šå¤§æ–‡ä»¶åˆå¹¶"""
    print(f"ğŸ”„ åˆå¹¶ {data_type} (å¤§æ–‡ä»¶ä¼˜åŒ–ç‰ˆ)...")
    
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
    
    if not os.path.exists(chunk_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {chunk_dir}")
        return False
    
    # è·å–chunkæ–‡ä»¶
    chunk_files = []
    for file in os.listdir(chunk_dir):
        if file.startswith("chunk_") and file.endswith(".pt"):
            try:
                num = int(file.split('_')[1].split('.')[0])
                size = os.path.getsize(os.path.join(chunk_dir, file))
                chunk_files.append((num, file, size))
            except:
                pass
    
    chunk_files.sort()
    print(f"ğŸ“¦ æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
    
    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    total_size = 0
    for num, filename, size in chunk_files:
        size_mb = size / (1024*1024)
        total_size += size
        print(f"   ğŸ“„ {filename}: {size_mb:.1f} MB")
    
    print(f"ğŸ“Š æ€»å¤§å°: {total_size / (1024*1024*1024):.2f} GB")
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    if os.path.exists(output_file):
        response = input(f"âš ï¸  {output_file} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–? (y/n): ")
        if response.lower() != 'y':
            return True
    
    # å¼€å§‹åˆå¹¶
    all_data = []
    total_dialogues = 0
    
    for i, (num, filename, size) in enumerate(chunk_files):
        chunk_path = os.path.join(chunk_dir, filename)
        size_mb = size / (1024*1024)
        
        print(f"\nğŸ“¦ å¤„ç† {filename} ({i+1}/{len(chunk_files)})...")
        print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")
        
        # è®¾ç½®è¶…æ—¶åŠ è½½
        start_time = time.time()
        
        try:
            print(f"   ğŸ”„ å¼€å§‹åŠ è½½... (è¶…æ—¶: {timeout_seconds}ç§’)")
            
            # å°è¯•åŠ è½½æ•°æ®
            data = torch.load(chunk_path, weights_only=True)
            
            load_time = time.time() - start_time
            dialogue_count = len(data)
            
            print(f"   âœ… åŠ è½½æˆåŠŸ!")
            print(f"   â±ï¸  åŠ è½½æ—¶é—´: {load_time:.1f}ç§’")
            print(f"   ğŸ“Š å¯¹è¯æ•°: {dialogue_count:,}")
            
            # æ·»åŠ åˆ°æ€»æ•°æ®
            print(f"   ğŸ”„ åˆå¹¶æ•°æ®...")
            all_data.extend(data)
            total_dialogues += dialogue_count
            
            print(f"   ğŸ“Š ç´¯è®¡å¯¹è¯æ•°: {total_dialogues:,}")
            
            # æ¸…ç†å†…å­˜
            del data
            gc.collect()
            
            print(f"   ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            load_time = time.time() - start_time
            print(f"   âŒ åŠ è½½å¤±è´¥ (ç”¨æ—¶ {load_time:.1f}ç§’): {e}")
            
            # è¯¢é—®æ˜¯å¦è·³è¿‡
            response = input(f"   æ˜¯å¦è·³è¿‡æ­¤æ–‡ä»¶ç»§ç»­? (y/n): ")
            if response.lower() != 'y':
                return False
            continue
    
    # ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœ...")
    print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {total_dialogues:,}")
    
    try:
        save_start = time.time()
        torch.save(all_data, output_file)
        save_time = time.time() - save_start
        
        print(f"âœ… ä¿å­˜æˆåŠŸ!")
        print(f"â±ï¸  ä¿å­˜æ—¶é—´: {save_time:.1f}ç§’")
        
        # éªŒè¯æ–‡ä»¶
        file_size = os.path.getsize(output_file) / (1024**3)
        print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶å¤§å°: {file_size:.2f} GB")
        
        # å¿«é€ŸéªŒè¯
        print(f"ğŸ” éªŒè¯æ–‡ä»¶...")
        verify_data = torch.load(output_file, weights_only=True)
        print(f"âœ… éªŒè¯æˆåŠŸ: {len(verify_data):,} ä¸ªå¯¹è¯")
        
        del all_data, verify_data
        gc.collect()
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False

class StreamMergerLegacy:
    """é—ç•™ç‰ˆæœ¬ï¼šæµå¼åˆå¹¶å™¨"""
    def __init__(self, data_type, max_workers=4):
        self.data_type = data_type
        self.max_workers = max_workers
        self.chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
        self.output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
        self.resume_file = os.path.join(config.LCCC_PROCESSED_PATH, f"merge_resume_{data_type}.json")
        self.temp_file = self.output_file + ".tmp"
        
    def get_chunk_files(self):
        """è·å–æ‰€æœ‰chunkæ–‡ä»¶"""
        if not os.path.exists(self.chunk_dir):
            return []
        
        chunk_files = []
        for file in os.listdir(self.chunk_dir):
            if file.startswith("chunk_") and file.endswith(".pt"):
                try:
                    chunk_num = int(file.split('_')[1].split('.')[0])
                    chunk_files.append((chunk_num, file))
                except:
                    pass
        
        return sorted(chunk_files, key=lambda x: x[0])
    
    def load_resume_state(self):
        """åŠ è½½æ–­ç‚¹ç»­ä¼ çŠ¶æ€"""
        if os.path.exists(self.resume_file):
            try:
                with open(self.resume_file, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {"processed_chunks": 0, "total_dialogues": 0}
    
    def save_resume_state(self, processed_chunks, total_dialogues):
        """ä¿å­˜æ–­ç‚¹ç»­ä¼ çŠ¶æ€"""
        state = {
            "processed_chunks": processed_chunks,
            "total_dialogues": total_dialogues,
            "timestamp": time.time()
        }
        with open(self.resume_file, 'w') as f:
            json.dump(state, f)
    
    def load_chunk_worker(self, chunk_info):
        """å·¥ä½œçº¿ç¨‹ï¼šåŠ è½½å•ä¸ªchunk"""
        chunk_num, chunk_file = chunk_info
        chunk_path = os.path.join(self.chunk_dir, chunk_file)
        
        try:
            data = torch.load(chunk_path, weights_only=True)
            return chunk_num, data, len(data)
        except Exception as e:
            print(f"âŒ åŠ è½½ {chunk_file} å¤±è´¥: {e}")
            return chunk_num, None, 0
    
    def append_to_file(self, new_data):
        """è¿½åŠ æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            # åŠ è½½ç°æœ‰æ•°æ®
            if os.path.exists(self.temp_file):
                existing_data = torch.load(self.temp_file, weights_only=True)
            else:
                existing_data = []
            
            # è¿½åŠ æ–°æ•°æ®
            existing_data.extend(new_data)
            
            # ä¿å­˜å›æ–‡ä»¶
            torch.save(existing_data, self.temp_file)
            
            # æ¸…ç†å†…å­˜
            del existing_data
            
        except Exception as e:
            print(f"âŒ è¿½åŠ æ•°æ®å¤±è´¥: {e}")
            raise
    
    def stream_merge(self):
        """æµå¼åˆå¹¶"""
        print(f"\nğŸ”„ å¼€å§‹æµå¼åˆå¹¶ {self.data_type}...")
        
        # è·å–chunkæ–‡ä»¶åˆ—è¡¨
        chunk_files = self.get_chunk_files()
        if not chunk_files:
            print(f"âŒ æœªæ‰¾åˆ°chunkæ–‡ä»¶")
            return False
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
        
        # åŠ è½½æ–­ç‚¹ç»­ä¼ çŠ¶æ€
        resume_state = self.load_resume_state()
        start_chunk = resume_state.get("processed_chunks", 0)
        total_dialogues = resume_state.get("total_dialogues", 0)
        
        if start_chunk > 0:
            print(f"ğŸ”„ ä»ç¬¬ {start_chunk} ä¸ªchunkç»§ç»­ (å·²å¤„ç† {total_dialogues:,} ä¸ªå¯¹è¯)")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¾“å‡ºæ–‡ä»¶
        if os.path.exists(self.output_file) and start_chunk == 0:
            response = input(f"âš ï¸  {self.data_type}.pt å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–? (y/n): ")
            if response.lower() != 'y':
                return False
        
        # åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶
        if start_chunk == 0:
            # åˆ›å»ºæ–°æ–‡ä»¶
            torch.save([], self.temp_file)
            print(f"ğŸ“„ åˆ›å»ºä¸´æ—¶æ–‡ä»¶: {self.temp_file}")
        
        # æµå¼å¤„ç†
        processed_chunks = start_chunk
        chunk_queue = queue.Queue(maxsize=self.max_workers * 2)
        
        # å¯åŠ¨åŠ è½½çº¿ç¨‹
        def chunk_loader():
            for i in range(start_chunk, len(chunk_files)):
                chunk_queue.put(chunk_files[i])
            chunk_queue.put(None)  # ç»“æŸæ ‡è®°
        
        loader_thread = threading.Thread(target=chunk_loader)
        loader_thread.start()
        
        # å¤„ç†chunk
        with tqdm(total=len(chunk_files) - start_chunk, desc=f"åˆå¹¶{self.data_type}") as pbar:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                
                while True:
                    chunk_info = chunk_queue.get()
                    if chunk_info is None:
                        break
                    
                    # æäº¤åŠ è½½ä»»åŠ¡
                    future = executor.submit(self.load_chunk_worker, chunk_info)
                    
                    try:
                        chunk_num, chunk_data, dialogue_count = future.result(timeout=60)
                        
                        if chunk_data is not None:
                            # è¿½åŠ åˆ°æ–‡ä»¶
                            self.append_to_file(chunk_data)
                            total_dialogues += dialogue_count
                            processed_chunks += 1
                            
                            # æ›´æ–°è¿›åº¦
                            pbar.update(1)
                            pbar.set_postfix({
                                'chunk': chunk_num,
                                'dialogues': f"{total_dialogues:,}"
                            })
                            
                            # å®šæœŸä¿å­˜æ–­ç‚¹
                            if processed_chunks % 10 == 0:
                                self.save_resume_state(processed_chunks, total_dialogues)
                        
                        # æ¸…ç†å†…å­˜
                        del chunk_data
                        
                    except Exception as e:
                        print(f"âŒ å¤„ç†chunkå¤±è´¥: {e}")
                        continue
        
        loader_thread.join()
        
        # å®Œæˆåˆå¹¶
        if os.path.exists(self.temp_file):
            # é‡å‘½åä¸´æ—¶æ–‡ä»¶
            if os.path.exists(self.output_file):
                os.remove(self.output_file)
            os.rename(self.temp_file, self.output_file)
            
            # æ¸…ç†æ–­ç‚¹æ–‡ä»¶
            if os.path.exists(self.resume_file):
                os.remove(self.resume_file)
            
            print(f"âœ… {self.data_type}.pt åˆå¹¶å®Œæˆ")
            print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {total_dialogues:,}")
            
            # éªŒè¯æ–‡ä»¶
            self.verify_merged_file()
            return True
        
        return False
    
    def verify_merged_file(self):
        """éªŒè¯åˆå¹¶åçš„æ–‡ä»¶"""
        try:
            data = torch.load(self.output_file, weights_only=True)
            file_size = os.path.getsize(self.output_file) / (1024**3)
            print(f"âœ… éªŒè¯æˆåŠŸ: {len(data):,} ä¸ªå¯¹è¯, {file_size:.2f} GB")
            del data
        except Exception as e:
            print(f"âŒ éªŒè¯å¤±è´¥: {e}")

def simple_merge_legacy(data_type):
    """é—ç•™ç‰ˆæœ¬ï¼šç®€å•åˆå¹¶"""
    print(f"ğŸ”„ ç®€å•åˆå¹¶ {data_type}...")
    
    chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
    output_file = os.path.join(config.LCCC_PROCESSED_PATH, f"{data_type}.pt")
    
    print(f"ğŸ“ chunkç›®å½•: {chunk_dir}")
    print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    if not os.path.exists(chunk_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨")
        return False
    
    # è·å–chunkæ–‡ä»¶
    chunk_files = []
    for file in os.listdir(chunk_dir):
        if file.startswith("chunk_") and file.endswith(".pt"):
            try:
                num = int(file.split('_')[1].split('.')[0])
                chunk_files.append((num, file))
            except:
                pass
    
    chunk_files.sort()
    print(f"ğŸ“¦ æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
    
    if not chunk_files:
        print(f"âŒ æ— chunkæ–‡ä»¶")
        return False
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    if os.path.exists(output_file):
        print(f"âš ï¸  è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨")
        return True
    
    # å¼€å§‹åˆå¹¶
    all_data = []
    
    for i, (num, filename) in enumerate(chunk_files):
        chunk_path = os.path.join(chunk_dir, filename)
        print(f"ğŸ“¦ å¤„ç† {filename} ({i+1}/{len(chunk_files)})...")
        
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            size_mb = os.path.getsize(chunk_path) / (1024*1024)
            print(f"   å¤§å°: {size_mb:.1f} MB")
            
            # åŠ è½½æ•°æ®
            print(f"   åŠ è½½ä¸­...")
            data = torch.load(chunk_path, weights_only=True)
            print(f"   âœ… åŠ è½½æˆåŠŸ: {len(data)} ä¸ªå¯¹è¯")
            
            # æ·»åŠ åˆ°æ€»æ•°æ®
            all_data.extend(data)
            print(f"   ğŸ“Š ç´¯è®¡: {len(all_data)} ä¸ªå¯¹è¯")
            
            # æ¸…ç†
            del data
            gc.collect()
            
            # æ¯å¤„ç†å‡ ä¸ªæ–‡ä»¶å°±å¼ºåˆ¶åƒåœ¾å›æ”¶
            if (i + 1) % 5 == 0:
                print(f"   ğŸ§¹ å¼ºåˆ¶åƒåœ¾å›æ”¶...")
                gc.collect()
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            return False
    
    # ä¿å­˜ç»“æœ
    print(f"ğŸ’¾ ä¿å­˜åˆ° {output_file}...")
    print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {len(all_data)}")
    
    try:
        torch.save(all_data, output_file)
        print(f"âœ… ä¿å­˜æˆåŠŸ")
        
        # éªŒè¯
        file_size = os.path.getsize(output_file) / (1024**3)
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} GB")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•° - é—ç•™åˆå¹¶è„šæœ¬çš„å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="é—ç•™åˆå¹¶è„šæœ¬")
    parser.add_argument("--method", choices=["simple", "stream", "large"], 
                       default="large", help="åˆå¹¶æ–¹æ³•")
    parser.add_argument("--dataset", choices=["train", "valid", "test"],
                       required=True, help="è¦åˆå¹¶çš„æ•°æ®é›†")
    parser.add_argument("--workers", type=int, default=2, help="å·¥ä½œçº¿ç¨‹æ•°")
    
    args = parser.parse_args()
    
    print("ğŸ”§ é—ç•™åˆå¹¶è„šæœ¬")
    print("=" * 40)
    
    if args.method == "simple":
        success = simple_merge_legacy(args.dataset)
    elif args.method == "stream":
        merger = StreamMergerLegacy(args.dataset, args.workers)
        success = merger.stream_merge()
    elif args.method == "large":
        success = merge_large_chunks_legacy(args.dataset)
    
    if success:
        print(f"âœ… {args.dataset} åˆå¹¶æˆåŠŸ")
    else:
        print(f"âŒ {args.dataset} åˆå¹¶å¤±è´¥")

if __name__ == "__main__":
    main()
