#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸…ç†æŸåçš„partialæ–‡ä»¶
"""

import os
import sys
import torch
import json
sys.path.append('.')
import config

def cleanup_partial_files():
    """æ¸…ç†æŸåçš„partialæ–‡ä»¶"""
    print("ğŸ§¹ æ¸…ç†æŸåçš„partialæ–‡ä»¶...")
    
    data_types = ["train", "valid", "test"]
    cleaned_files = []
    
    for data_type in data_types:
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
        if not os.path.exists(chunk_dir):
            continue
            
        print(f"\nğŸ“ æ£€æŸ¥ {data_type} ç›®å½•: {chunk_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰partialæ–‡ä»¶
        partial_files = []
        for file in os.listdir(chunk_dir):
            if file.startswith("partial_") and file.endswith(".pt"):
                partial_files.append(file)
        
        if not partial_files:
            print(f"   âœ… æ— partialæ–‡ä»¶")
            continue
            
        print(f"   ğŸ“¦ æ‰¾åˆ° {len(partial_files)} ä¸ªpartialæ–‡ä»¶")
        
        for partial_file in partial_files:
            partial_path = os.path.join(chunk_dir, partial_file)
            print(f"   ğŸ” æ£€æŸ¥: {partial_file}")
            
            try:
                # å°è¯•åŠ è½½æ–‡ä»¶
                data = torch.load(partial_path, weights_only=True)
                print(f"      âœ… æ–‡ä»¶æ­£å¸¸ï¼ŒåŒ…å« {len(data)} ä¸ªå¯¹è¯")
            except Exception as e:
                print(f"      âŒ æ–‡ä»¶æŸå: {e}")
                print(f"      ğŸ—‘ï¸  åˆ é™¤æŸåæ–‡ä»¶: {partial_file}")
                
                # å¤‡ä»½æ–‡ä»¶å
                backup_path = partial_path + ".backup"
                if os.path.exists(backup_path):
                    os.remove(backup_path)
                
                # é‡å‘½åä¸ºå¤‡ä»½
                os.rename(partial_path, backup_path)
                cleaned_files.append(partial_path)
                
                print(f"      ğŸ’¾ å·²å¤‡ä»½ä¸º: {partial_file}.backup")
    
    return cleaned_files

def check_resume_files():
    """æ£€æŸ¥resumeæ–‡ä»¶çŠ¶æ€"""
    print(f"\nğŸ“‹ æ£€æŸ¥resumeæ–‡ä»¶çŠ¶æ€...")
    
    data_types = ["train", "valid", "test"]
    
    for data_type in data_types:
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
        resume_file = os.path.join(chunk_dir, f"resume_{data_type}.json")
        
        if os.path.exists(resume_file):
            try:
                with open(resume_file, 'r') as f:
                    resume_data = json.load(f)
                print(f"   ğŸ“„ {data_type}: {resume_data}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰partialæ ‡è®°ä½†partialæ–‡ä»¶ä¸å­˜åœ¨
                if resume_data.get('has_partial', False):
                    chunk_index = resume_data.get('chunk_index', 0)
                    partial_file = os.path.join(chunk_dir, f"partial_{chunk_index}.pt")
                    if not os.path.exists(partial_file):
                        print(f"      âš ï¸  æ ‡è®°æœ‰partialä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ›´æ–°resumeæ–‡ä»¶")
                        resume_data['has_partial'] = False
                        with open(resume_file, 'w') as f:
                            json.dump(resume_data, f)
                        print(f"      âœ… å·²æ›´æ–°resumeæ–‡ä»¶")
                        
            except Exception as e:
                print(f"   âŒ {data_type} resumeæ–‡ä»¶æŸå: {e}")
        else:
            print(f"   ğŸ“„ {data_type}: æ— resumeæ–‡ä»¶")

def check_chunk_integrity():
    """æ£€æŸ¥chunkæ–‡ä»¶å®Œæ•´æ€§"""
    print(f"\nğŸ” æ£€æŸ¥chunkæ–‡ä»¶å®Œæ•´æ€§...")
    
    data_types = ["train", "valid", "test"]
    
    for data_type in data_types:
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
        if not os.path.exists(chunk_dir):
            continue
            
        chunk_files = [f for f in os.listdir(chunk_dir) if f.startswith("chunk_") and f.endswith(".pt")]
        if not chunk_files:
            print(f"   ğŸ“¦ {data_type}: æ— chunkæ–‡ä»¶")
            continue
            
        # è·å–chunkç¼–å·
        chunk_numbers = []
        for f in chunk_files:
            try:
                num = int(f.split('_')[1].split('.')[0])
                chunk_numbers.append(num)
            except:
                pass
        
        if chunk_numbers:
            chunk_numbers.sort()
            print(f"   ğŸ“¦ {data_type}: {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
            print(f"      ç¼–å·èŒƒå›´: {min(chunk_numbers)} - {max(chunk_numbers)}")
            
            # æ£€æŸ¥è¿ç»­æ€§
            expected = list(range(min(chunk_numbers), max(chunk_numbers) + 1))
            missing = set(expected) - set(chunk_numbers)
            if missing:
                print(f"      âš ï¸  ç¼ºå¤±ç¼–å·: {sorted(missing)}")
            else:
                print(f"      âœ… ç¼–å·è¿ç»­")
            
            # æ£€æŸ¥æœ€åå‡ ä¸ªæ–‡ä»¶çš„å®Œæ•´æ€§
            last_chunks = sorted(chunk_numbers)[-3:]  # æ£€æŸ¥æœ€å3ä¸ª
            for chunk_num in last_chunks:
                chunk_file = os.path.join(chunk_dir, f"chunk_{chunk_num}.pt")
                try:
                    data = torch.load(chunk_file, weights_only=True)
                    print(f"      âœ… chunk_{chunk_num}.pt: {len(data)} ä¸ªå¯¹è¯")
                except Exception as e:
                    print(f"      âŒ chunk_{chunk_num}.pt æŸå: {e}")

def main():
    print("ğŸ§¹ Partialæ–‡ä»¶æ¸…ç†å·¥å…·")
    print("=" * 50)
    
    # 1. æ¸…ç†æŸåçš„partialæ–‡ä»¶
    cleaned_files = cleanup_partial_files()
    
    # 2. æ£€æŸ¥resumeæ–‡ä»¶
    check_resume_files()
    
    # 3. æ£€æŸ¥chunkæ–‡ä»¶å®Œæ•´æ€§
    check_chunk_integrity()
    
    # 4. æ€»ç»“
    print(f"\nğŸ“Š æ¸…ç†æ€»ç»“:")
    if cleaned_files:
        print(f"   ğŸ—‘ï¸  æ¸…ç†äº† {len(cleaned_files)} ä¸ªæŸåçš„partialæ–‡ä»¶")
        for file in cleaned_files:
            print(f"      - {file}")
        print(f"   ğŸ’¾ åŸæ–‡ä»¶å·²å¤‡ä»½ä¸º .backup")
    else:
        print(f"   âœ… æœªå‘ç°æŸåçš„partialæ–‡ä»¶")
    
    print(f"\nğŸ’¡ å»ºè®®:")
    print(f"1. ç°åœ¨å¯ä»¥å®‰å…¨åœ°ç»§ç»­è¿è¡Œæ•°æ®è½¬æ¢")
    print(f"2. ç³»ç»Ÿä¼šä»æ­£ç¡®çš„ä½ç½®ç»§ç»­å¤„ç†")
    print(f"3. å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯èƒ½æ˜¯ç£ç›˜ç©ºé—´æˆ–å†…å­˜ä¸è¶³")
    
    print(f"\nğŸš€ ç»§ç»­å¤„ç†å‘½ä»¤:")
    print(f"python -m src.prepare_binary_data --num_workers=2")

if __name__ == "__main__":
    main()
