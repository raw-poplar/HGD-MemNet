#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§¦å‘åŸå§‹çš„åˆå¹¶é€»è¾‘
"""

import os
import sys
import glob
sys.path.append('.')
import config
from src.prepare_binary_data import merge_chunks

def cleanup_and_merge():
    """æ¸…ç†partialæ–‡ä»¶å¹¶è§¦å‘åˆå¹¶"""
    print("ğŸ”§ æ¸…ç†partialæ–‡ä»¶å¹¶è§¦å‘åˆå¹¶...")
    
    data_types = ["train", "valid", "test"]
    
    for data_type in data_types:
        chunk_dir = os.path.join(config.LCCC_PROCESSED_PATH, data_type)
        if not os.path.exists(chunk_dir):
            print(f"âŒ {data_type} ç›®å½•ä¸å­˜åœ¨")
            continue
            
        print(f"\nğŸ“ å¤„ç† {data_type}:")
        
        # 1. æ¸…ç†partialæ–‡ä»¶
        partial_pattern = os.path.join(chunk_dir, "partial_*.pt")
        partial_files = glob.glob(partial_pattern)
        
        if partial_files:
            print(f"   ğŸ”„ æ‰¾åˆ° {len(partial_files)} ä¸ªpartialæ–‡ä»¶")
            for partial_file in partial_files:
                try:
                    partial_index = int(os.path.basename(partial_file).split('_')[1].split('.')[0])
                    target_chunk = os.path.join(chunk_dir, f"chunk_{partial_index}.pt")
                    
                    if os.path.exists(target_chunk):
                        print(f"   âš ï¸  chunk_{partial_index}.pt å·²å­˜åœ¨ï¼Œåˆ é™¤partialæ–‡ä»¶")
                        os.remove(partial_file)
                    else:
                        print(f"   ğŸ”„ é‡å‘½å: partial_{partial_index}.pt -> chunk_{partial_index}.pt")
                        os.rename(partial_file, target_chunk)
                        
                except Exception as e:
                    print(f"   âŒ å¤„ç† {partial_file} å¤±è´¥: {e}")
        else:
            print(f"   âœ… æ— partialæ–‡ä»¶")
        
        # 2. æ£€æŸ¥chunkæ–‡ä»¶
        chunk_files = glob.glob(os.path.join(chunk_dir, "chunk_*.pt"))
        print(f"   ğŸ“¦ æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
        
        if len(chunk_files) == 0:
            print(f"   âŒ æ— chunkæ–‡ä»¶å¯åˆå¹¶")
            continue
        
        # 3. æ£€æŸ¥æ˜¯å¦å·²æœ‰åˆå¹¶æ–‡ä»¶
        merged_file = os.path.join(chunk_dir, f"{data_type}.pt")
        if os.path.exists(merged_file):
            response = input(f"   âš ï¸  {data_type}.pt å·²å­˜åœ¨ï¼Œæ˜¯å¦é‡æ–°åˆå¹¶? (y/n): ")
            if response.lower() != 'y':
                print(f"   â­ï¸  è·³è¿‡ {data_type}")
                continue
        
        # 4. æ‰§è¡Œåˆå¹¶
        try:
            print(f"   ğŸ”„ å¼€å§‹åˆå¹¶...")
            merge_chunks(chunk_dir, data_type, delete_chunks_after_merge=False)  # ä¸åˆ é™¤chunkæ–‡ä»¶
            print(f"   âœ… {data_type} åˆå¹¶å®Œæˆ")
        except Exception as e:
            print(f"   âŒ {data_type} åˆå¹¶å¤±è´¥: {e}")

def verify_merged_files():
    """éªŒè¯åˆå¹¶åçš„æ–‡ä»¶"""
    print(f"\nğŸ“Š éªŒè¯åˆå¹¶åçš„æ–‡ä»¶...")
    
    data_types = ["train", "valid", "test"]
    
    for data_type in data_types:
        merged_file = os.path.join(config.LCCC_PROCESSED_PATH, data_type, f"{data_type}.pt")
        
        if os.path.exists(merged_file):
            try:
                import torch
                data = torch.load(merged_file, weights_only=True)
                size_gb = os.path.getsize(merged_file) / (1024**3)
                print(f"   âœ… {data_type}.pt: {len(data):,} ä¸ªå¯¹è¯, {size_gb:.2f} GB")
            except Exception as e:
                print(f"   âŒ {data_type}.pt éªŒè¯å¤±è´¥: {e}")
        else:
            print(f"   âŒ {data_type}.pt ä¸å­˜åœ¨")

if __name__ == "__main__":
    print("ğŸ”§ è§¦å‘åŸå§‹åˆå¹¶é€»è¾‘")
    print("=" * 40)
    
    cleanup_and_merge()
    verify_merged_files()
    
    print(f"\nğŸ¯ å®Œæˆ!")
    print(f"ğŸ’¡ åˆå¹¶åçš„æ–‡ä»¶ä½ç½®:")
    print(f"   ğŸ“ F:/modelTrain/data/lccc_processed/train/train.pt")
    print(f"   ğŸ“ F:/modelTrain/data/lccc_processed/valid/valid.pt")
    print(f"   ğŸ“ F:/modelTrain/data/lccc_processed/test/test.pt")
