1. 权重剪枝，现在的模型架构是动态神经元之间两两相连，假如令模型的权重在训练过程中如果小于一定值（或者不满足一定条件），就丢弃这个权重，即这两个神经元之间没有直接联系，不能直接进行数据交换了；
2. 神经元剪枝，为所有的神经元增加一个能够令这个神经元对所有与这个神经元相连接的其他神经元进行评价的方法，也就是某个神经元能够从所有与它相连接的其他神经元那里获取自己的评价分数，而当这个评价分数低于一定值我们就重置这个神经元；
2.1. 对于第2点我们可以不单单使用一个神经元，我们可以使用一组数量的神经元，统计出这一组数量的神经元的输入输出，然后按照第2点来按组评价，如果评价分数低则重置这一组神经元的内部权重；
3. 当动态神经组中的神经元换成其他架构的神经网络后的训练方法改进，我们先拿到一个训练好的hgd-memnet，取出其中的一个或数个神经元，然后取一个其他架构的神经网络（这里我们先使用transformer架构网络作说明），我们使用hgd-memnet取出的神经元对transformer网络做强化学习，训练完后使用transformer网络替换掉原来的hgd-memnet的神经元，成为transformer神经元，然后接着学习
3.1. 在3的基础上，我们可以尝试将transformer神经元做的比较大并且多引入几个transformer神经元，我们将这些比较大的神经元合起来成为动态中枢神经组，然后引入动态模式和缓态模式，动态模式就是动态神经组全部神经元都处在运行状态，缓态模式则是只有动态中枢神经组处于运行状态。
4. 饥饿式训练，当模型在按当前的训练方法训练到一半，拥有一定的推理、思考和对话能力后，我将一长段对话的数据集按对话拆开，一句话一句话喂给它，然后每喂一句话就间隔很长时间给模型充足甚至过于丰富的时间思考，令模型在大部分时间都处在“饿着”的状态来训练模型的思考能力，
4.1 饱腹式训练，与饥饿式训练相对的，在模型的思考周期很短甚至没有思考步骤的时间内喂给模型大量的数据，这样来锻炼模型的信息提取能力
5. 中枢神经对话演进。在模型长时间不接收输入的时候停止其他神经元的数据交换并令其他神经元都向中枢神经元做一次数据交换，然后中枢神经元之间不停止数据交换，这是中枢神经元的作用。我们在对话中，在模型推理的过程中，将数据交换次数最多的神经元（或者另起一个定义中枢神经元的标准，现在还没想好）定义为中枢神经元这个样子。
6. 动态对话剪枝与再生成。顾名思义，就是在推理的时候为模型开放一小部分权重，模型可以根据对话演进自己修改这部分的权重，修改方式使用剪枝+再生成的方式做删除修改与生成
1.1 权重剪枝的新的想法。为模型的动态神经组的所有权重乘上一个偏置，对于一个神经元，它的所有 权重*偏置（这是权重的选择概率公式，因为有温度的影响后面估计是要修改的，这里是不是找个学数学的搞权重概率公式会好点？） 的和为1，当神经元在训练的时候选择向这个权重所代表的神经元传输数据，则这个权重的偏置增加，其他偏置减少，如果有偏置的值到0（或者低于我们设的值比如1e-4这样的），我们就删除这个权重
7. 为神经元做个约束，这是架构上的改动，我们可以为每个神经元加上距离限制，神经元只向在一定距离范围内的神经元作数据传输，这样在一个小范围内的神经元思考的数据会与其他位置的神经元可以不同，据此来做神经元的分化，模型可以思考到更多不同的东西，然后静态神经组本来的与动态神经组的部分连接需要变成100%的全连接
8. 为模型的动态神经组加上一个树做模型的记忆树，模型可以为记忆树做增删查改，记忆树的每个节点都包含数据，节点的子节点的数据都是当前节点数据的分化与详细化，节点的父节点都是包括节点在内的所有节点的汇总，我们可以取动态神经组的一部分比如30%的神经元随机的散布在记忆树的各个节点，这些神经元可以根据输入到神经元的信息来查找整个动态神经组可能需要的信息，在记忆树中每个节点需要与2个动态神经组的神经元连接，一个神经元用来接收外部其他神经元的输入并总结需要什么信息，另一个神经元则在记忆树中查找信息，查找信息神经元拥有对记忆树增删查改的能力但是需要接受神经元的同意，查找神经元每查找到一个记忆树的节点就获取当前记忆树及其所有子节点的信息，如果需要则交给接受神经元输出到其他节点，并且为这些信息打上特殊标识象征着是从记忆树中获取的数据
8.1 关于记忆树的数据结构的想法，其实可以试试线段树，我们可以设定线段树的长度为比方说0-1e9，将部分放在浮点的信息移到整数部分，在使用记忆数据布满（线段树无限分割，这里用词仅供参考）线段树后，我们可以使用线段树的值映射与数据来训练一个新的模型，当我们在线段树上随便选择一个点，这个模型就可以尝试根据原先线段树的信息来预测这个位置的信息是什么，虽然不知道这样有什么用但是想到了这些